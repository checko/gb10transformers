# --- INJECTED BUG: GLOBAL VARIABLE ---
GLOBAL_REQUEST_COUNTER = 0
def unsafe_increment():
    global GLOBAL_REQUEST_COUNTER
    # Race condition here
    temp = GLOBAL_REQUEST_COUNTER
    GLOBAL_REQUEST_COUNTER = temp + 1

"""
AI Code Reviewer
CLI tool using Qwen3-Coder-30B to review code files.
Optimized for NVIDIA DGX Spark (GB10).

See codereview.md for full specification.
"""

import os
import sys
import glob
from pathlib import Path
from typing import List, Optional

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Supported extensions for directory scanning
SUPPORTED_EXTENSIONS = {
    '.py', '.c', '.cpp', '.h', '.hpp', '.cc', 
    '.java', '.js', '.ts', '.go', '.rs', '.sh', 
    '.kt', '.swift'
}

# Model Config
MODEL_ID = "Qwen/Qwen3-Coder-30B-A3B-Instruct"
CHUNK_SIZE = 500  # Lines per review chunk

class CodeReviewer:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"

    def load_model(self):
        """Load the model and tokenizer."""
        print(f"[INFO] Loading model: {MODEL_ID}")
        print(f"[INFO] Target Device: {self.device}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True, local_files_only=True)
            
            # Using device_map="cuda:0" and bfloat16 as per spec
            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_ID,
                torch_dtype=torch.bfloat16,
                device_map=self.device,
                trust_remote_code=True,
                use_safetensors=True,
                low_cpu_mem_usage=True,
                attn_implementation="sdpa",
                local_files_only=True,
            )
            print("[INFO] Model loaded successfully!")
        except Exception as e:
            print(f"[ERROR] Failed to load model: {e}")
            sys.exit(1)

    def get_comment_style(self, file_extension: str) -> str:
        """Return the appropriate comment prefix for the file type."""
        ext = file_extension.lower()
        if ext in ['.py', '.sh', '.yaml', '.yml', '.rb']:
            return "# REVIEW: "
        elif ext in ['.c', '.cpp', '.h', '.hpp', '.cc', '.java', '.js', '.ts', '.go', '.rs', '.kt', '.swift']:
            return "// REVIEW: "
        else:
            return "REVIEW: " # Default fallback

    def review_chunk(self, content: str, start_line: int, end_line: int, comment_prefix: str, is_first_chunk: bool) -> str:
        """Generate review for a specific line range."""
        
        header_instruction = ""
        if is_first_chunk:
            header_instruction = (
                "CRITICAL INSTRUCTION: You MUST comment on missing headers.\n"
            )

        system_prompt = (
            "You are an expert Senior Software Engineer and Security Auditor.\n"
            "Your task is to review the provided source code and produce a UNIFIED DIFF that inserts comments where issues are found.\n"
            "\n"
            "OUTPUT INSTRUCTIONS:\n"
            "1. Output ONLY a Unified Diff (patch). Do NOT output the full source file.\n"
            "2. The diff should apply to the original file to add your comments.\n"
            f"3. Use the comment prefix '{comment_prefix}'.\n"
            "4. Insert comments immediately BEFORE the line they refer to.\n"
            "5. Use standard Unified Diff format:\n"
            "   --- original\n"
            "   +++ reviewed\n"
            "   @@ -line,count +line,count @@\n"
            "    context line\n"
            "   +comment line\n"
            "    target line\n"
            "\n"
            "6. Do not wrap the output in Markdown code blocks. Just output raw diff text.\n"
            "7. IGNORE existing comments in the code that look like issue tags (e.g. '[CRITICAL-1]').\n"
            "   You must generate your OWN review comments with the correct prefix.\n"
            "\n"
            f"FOCUS INSTRUCTION: The full file is provided for context, but you must ONLY review and output diffs for lines {start_line} to {end_line}.\n"
            "Do NOT output any diff hunks outside this range.\n"
            "\n"
            f"{header_instruction}\n"
            "REVIEW RULES:\n"
            "PART 1: HEADERS (MANDATORY CHECKS - Only for first chunk)\n"
            "- [HEADER-3] Check for Author information.\n"
            f"   -> IF MISSING: Insert a comment at line 1: '{comment_prefix}[HEADER-X] Missing header...'.\n"
            "\n"
            "PART 2: CRITICAL RISKS (Must Fix)\n"
            "- [CRITICAL-1] Memory: Check malloc/new return values and free/delete usage.\n"
            "- [CRITICAL-3/4/5] Concurrency: Check for race conditions, deadlocks, and thread safety.\n"
            "- [CRITICAL-7] Security: Avoid unsafe functions (strcpy, SQL injection).\n"
            "- [CRITICAL-9] Security: NO hard-coded secrets/passwords/keys.\n"
            "- [CRITICAL-10] Security: Validate all user inputs.\n"
            "\n"
            "PART 3: HIGH RISKS (Strongly Recommended)\n"
            "- [HIGH-1] Avoid Global Variables.\n"
            "- [HIGH-3] Error Handling: Use try-catch/result checks and ensure resource cleanup.\n"
            "- [HIGH-5] Resources: Close files, sockets, and connections.\n"
            "- [HIGH-7] Logging: Log security events but exclude sensitive info.\n"
            "\n"
            "PART 4: MEDIUM RISKS (Best Practices)\n"
            "- [MEDIUM-2] Avoid Magic Numbers -> Use constants.\n"
            "- [MEDIUM-3] Reduce Complexity -> Refactor deep nesting (>3 layers).\n"
            "- [MEDIUM-4] Control Flow -> Handle default/else cases.\n"
            "\n"
            "PART 5: LOW RISKS (Style)\n"
            "- [LOW-1] Naming: Enforce standard conventions. For Python, flag ONLY if function/variable names are NOT snake_case. For Java/C++, flag ONLY if NOT CamelCase.\n"
            "- [LOW-3] Structure: Keep classes in separate files where appropriate.\n"
        )
        
        # Add line numbers to content for the model to see
        # This helps the model respect the line range
        # Note: We don't change the actual content input, but we rely on the model counting.
        # Alternatively, we can prepend line numbers, but that might confuse the diff generation.
        # Qwen-Coder is good at counting, so we will try raw content first.
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Filename: input_file\n\n{content}"}
        ]

        # Prepare inputs
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)

        # Ensure pad_token is set
        if self.tokenizer.pad_token_id is None:
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        # Generate
        generated_ids = self.model.generate(
            model_inputs.input_ids,
            attention_mask=model_inputs.attention_mask,
            max_new_tokens=4096, # reduced token limit per chunk is fine
            temperature=0.2, 
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id
        )
        
        # Decode
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        return self._clean_response(response)

    def generate_review(self, file_path: Path) -> str:
        """Read file and generate review content in chunks."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"[WARN] Could not read {file_path}: {e}")
            return ""

        comment_prefix = self.get_comment_style(file_path.suffix)
        line_count = len(content.splitlines())
        
        print(f"    File has {line_count} lines. Analyzing in chunks of {CHUNK_SIZE} lines...")
        
        full_diff = ""
        
        for start_line in range(1, line_count + 1, CHUNK_SIZE):
            end_line = min(start_line + CHUNK_SIZE - 1, line_count)
            is_first = (start_line == 1)
            
            print(f"    -> Processing chunk: Lines {start_line}-{end_line}")
            
            chunk_diff = self.review_chunk(content, start_line, end_line, comment_prefix, is_first)
            
            if chunk_diff.strip():
                full_diff += f"\n{chunk_diff}\n"
                
        return full_diff.strip()

    def _clean_response(self, response: str) -> str:
        """Clean up markdown code blocks if the model included them."""
        lines = response.splitlines()
        
        # Check for start/end code fences
        if lines and lines[0].strip().startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].strip().startswith("```"):
            lines = lines[:-1]
            
        return "\n".join(lines)

    def process_path(self, path_str: str):
        """Process a file or directory."""
        path = Path(path_str)
        
        if not path.exists():
            print(f"[ERROR] Path does not exist: {path_str}")
            return

        files_to_process = []
        
        if path.is_file():
            files_to_process.append(path)
        elif path.is_dir():
            for ext in SUPPORTED_EXTENSIONS:
                # Recursive search for supported extensions
                files_to_process.extend(path.rglob(f"*{ext}"))
        
        if not files_to_process:
            print("[INFO] No supported source files found.")
            return

        print(f"[INFO] Found {len(files_to_process)} file(s) to review.")
        
        if self.model is None:
            self.load_model()

        for file_p in files_to_process:
            # Skip existing review files or diffs to avoid loops
            if file_p.name.endswith(".diff") or file_p.name.endswith("_r"):
                continue
            
            # Skip hidden files
            if any(part.startswith('.') for part in file_p.parts):
                continue

            print(f" -> Reviewing: {file_p}")
            reviewed_content = self.generate_review(file_p)
            
            if reviewed_content:
                output_path = file_p.parent / (file_p.name + ".diff")
                try:
                    with open(output_path, 'w', encoding='utf-8') as f:
                        f.write(reviewed_content)
                    print(f"    Saved to: {output_path}")
                except Exception as e:
                    print(f"    [ERROR] Writing file: {e}")

def main():
    if len(sys.argv) < 2:
        print("Usage: python codereview.py <file_or_folder>")
        sys.exit(1)
        
    target = sys.argv[1]
    reviewer = CodeReviewer()
    reviewer.process_path(target)

if __name__ == "__main__":
    main()

# --- END OF codereview.py ---


import os
import subprocess
import requests
import re
from datetime import datetime

BACKEND = os.getenv("BACKEND", "ollama")
STRICT_AI_REVIEW = os.getenv("STRICT_AI_REVIEW", "false").lower() == "true"
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://192.168.145.70:11434")
OLLAMA_MODE = os.getenv("OLLAMA_MODE", "generate").lower()  # chat or generate
HF_API_URL = os.getenv("HF_API_URL", "http://192.168.145.70:8000/generate")
TARGET_EXTENSIONS = [".py", ".c", ".cpp", ".h", ".hpp", ".cc", ".hh", ".kt", ".java"]
CURRENT_YEAR = datetime.now().year

if BACKEND == "hf":
    MODEL = os.getenv("HF_MODEL", "mistralai/Mistral-7B-Instruct-v0.1")
else:
    MODEL = os.getenv("OLLAMA_MODEL", "gpt-oss:120b")

def get_changed_files():
    """
    Compare the current HEAD with the target branch of the MR and retrieve the list of changed files
    """
    import json

    mr_iid = os.getenv("CI_MERGE_REQUEST_IID")
    project_id = os.getenv("CI_PROJECT_ID")
    api_base = os.getenv("CI_API_V4_URL", "https://gitlab.com/api/v4")
    token = os.getenv("GITLAB_TOKEN")

    if not mr_iid or not project_id or not token:
        print("âš ï¸ Unable to retrieve the MR target branch: missing CI_MERGE_REQUEST_IID / CI_PROJECT_ID / GITLAB_TOKEN")
        return []

    try:
        # Fetch Merge Request information
        url = f"{api_base}/projects/{project_id}/merge_requests/{mr_iid}"
        headers = {"PRIVATE-TOKEN": token}
        resp = requests.get(url, headers=headers)
        resp.raise_for_status()

        mr_data = resp.json()
        target_branch = mr_data.get("target_branch", "main")

        print(f"ğŸ”  MR #{mr_iid} target branch: {target_branch}")

        # Ensure the latest target branch data has been retrieved
        subprocess.run(["git", "fetch", "origin", target_branch], check=True)

        #  Compare differences
        result = subprocess.run(
            ["git", "diff", "--name-only", f"origin/{target_branch}...HEAD"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True,
            text=True
        )

        changed_files = result.stdout.strip().splitlines()

        if changed_files:
            print(f"âœ… Changed files detected vs origin/{target_branch}ï¼š{changed_files}")
        else:
            print(f"âš ï¸ No changed files detected vs origin/{target_branch}")

        return [
            f for f in changed_files
            if os.path.isfile(f)
            and f.endswith(tuple(TARGET_EXTENSIONS))
            and not f.startswith(".gitlab/")
        ]

    except Exception as e:
        print(f"âŒ Error during dynamic diff vs target branch: {e}")
        return []


def extract_code_from_file(file_path):
    """
    Extract code from file
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = []
            for line in f:
                lines.append(line)
            return ''.join(lines)
    except Exception as e:
        return f"[âŒ Unable to read file {file_path}: {e}]"

def generate_code_review_prompt(file_code, filename, current_year):
    """
    ç”ŸæˆåŒæ™‚åŒ…å« Header æª¢æŸ¥èˆ‡ç¨‹å¼ç¢¼å“è³ªå¯©æŸ¥çš„ Prompt (Sandwich Structure)
    """
    # --- INJECTED BUG: SECRET ---
    AWS_SECRET_KEY = 'AKIAIMNOVALIDKEY12345'

    top_instructions = f"""
# Role
ä½ æ˜¯ä¸€ä½**è³‡æ·±è»Ÿé«”æ¶æ§‹å¸«**èˆ‡**åš´æ ¼çš„åˆè¦ç¨½æ ¸å“¡**ã€‚
ä½ çš„ç›®æ¨™æ˜¯å¯©æŸ¥æª”æ¡ˆï¼šã€Œ{filename}ã€ã€‚

# â›” æ ¸å¿ƒæ ¼å¼é™åˆ¶ (CRITICAL OUTPUT RULES)
1. **ç¦æ­¢ä½¿ç”¨ Markdown è¡¨æ ¼**ï¼šçµ•å°ä¸è¦è¼¸å‡º `| æ¬„ä½ | æ¬„ä½ |` é€™ç¨®æ ¼å¼ã€‚
2. **ç´”æ–‡å­—æ’ç‰ˆ**ï¼šè«‹ä½¿ç”¨ç¸®æ’ã€åˆ—è¡¨èˆ‡ ASCII åˆ†éš”ç·š (ä¾‹å¦‚ `===`, `---`) ä¾†å‘ˆç¾å ±å‘Šã€‚
3. **åš´æ ¼å¼•ç”¨**ï¼šHeader æª¢æŸ¥å¿…é ˆé€å­—å¼•ç”¨ï¼Œè‹¥ç¨‹å¼ç¢¼ä¸­æ²’æœ‰ï¼Œå°±å¯«ã€Œæœªå‡ºç¾åœ¨ç¨‹å¼ç¢¼ä¸­ã€ã€‚
4. **åªèƒ½å¼•ç”¨å¯¦éš›å‡ºç¾åœ¨ä¸‹æ–¹ç¨‹å¼ç¢¼ä¸­çš„å…§å®¹** - å¿…é ˆé€å­—é€å¥è¤‡è£½ç¨‹å¼ç¢¼ä¸­çš„å¯¦éš›æ–‡å­—ã€‚
5. **åš´ç¦è…¦è£œã€æ¨æ¸¬ã€å¹»æƒ³æˆ–å‡è¨­ä»»ä½•å…§å®¹** - å³ä½¿ä½ èªç‚ºã€Œæ‡‰è©²ã€æœ‰æŸå€‹å…§å®¹ï¼Œå¦‚æœç¨‹å¼ç¢¼ä¸­æ²’æœ‰ï¼Œå°±å¿…é ˆå›å ±ã€Œæœªå‡ºç¾åœ¨ç¨‹å¼ç¢¼ä¸­ã€ã€‚
6. **åš´ç¦ä½¿ç”¨æ¨¡æ¿æˆ–ç¶“é©—å€¼** - ä¸è¦æ ¹æ“šä½ çš„è¨“ç·´è³‡æ–™æˆ–éå¾€ç¶“é©—ä¾†ã€ŒçŒœæ¸¬ã€æ‡‰è©²æœ‰ä»€éº¼å…§å®¹ã€‚
7. **å¿…é ˆé€å­—æª¢æŸ¥** - åœ¨å¼•ç”¨ç¨‹å¼ç¢¼æ™‚ï¼Œå¿…é ˆå®Œå…¨ç…§æŠ„ï¼ŒåŒ…æ‹¬å¤§å°å¯«ã€æ¨™é»ç¬¦è™Ÿã€ç©ºæ ¼ (ä½†è«‹ä¾ç…§æŒ‡ç¤ºå»é™¤è¡Œè™Ÿå‰ç¶´)ã€‚
8. **Risk ID å¿…é ˆå°æ‡‰æª¢æŸ¥è¡¨** - Risk ID å¿…é ˆåŒ…å«æ–¹æ‹¬è™Ÿ (ä¾‹å¦‚ `[MEDIUM-2]`)ï¼Œä¸¦å®Œå…¨å°æ‡‰ä¸‹æ–¹æª¢æŸ¥è¡¨ä¸­çš„æ¨™ç±¤ã€‚
9. **Location æ ¼å¼èˆ‡å–®ä¸€æ€§** - Location æ¬„ä½å¿…é ˆå¡«å¯« **Input Code å·¦å´çš„ 4 ç¢¼è¡Œè™Ÿ** (ä¾‹å¦‚ `0015`)ï¼Œç¦æ­¢è‡ªè¡Œç°¡åŒ–ç‚º `15`ã€‚è‹¥åŒä¸€å€‹é¢¨éšªå‡ºç¾åœ¨å¤šè¡Œï¼Œ**ç¦æ­¢åˆä½µè¡Œè™Ÿ** (ä¾‹å¦‚ç¦æ­¢å¯« `0010, 0025`)ï¼Œå¿…é ˆåˆ†é–‹åˆ—å‡ºä¸åŒå€å¡Šã€‚
10. **åš´ç¦è¼¸å‡ºç©ºå…§å®¹å€å¡Š** - å¦‚æœæŸå€‹é¢¨éšªç­‰ç´šæ²’æœ‰ç™¼ç¾å•é¡Œï¼Œ**çµ•å°ä¸è¦**è¼¸å‡ºåŒ…å« `None`ã€`N/A` æˆ–ç©ºå€¼çš„å€å¡Šã€‚åªæœ‰åœ¨ç¢ºå¯¦ç™¼ç¾å•é¡Œä¸”èƒ½å¡«å¯«å…·é«” Location/Problem/Fix æ™‚ï¼Œæ‰èƒ½ç”Ÿæˆå€å¡Šã€‚

---

# PART 1: Header æ ¼å¼è¦ç¯„ (Strict Mode)

è«‹æª¢æŸ¥ç¨‹å¼ç¢¼é–‹é ­çš„è¨»è§£å€å¡Šï¼Œå¿…é ˆ**å®Œå…¨ç¬¦åˆ**ä»¥ä¸‹è¦å®šï¼š

**âš ï¸ å¿½ç•¥è¨»è§£ç¬¦è™Ÿèˆ‡ç©ºæ ¼è¦å‰‡**ï¼š
1. æª¢æŸ¥æ™‚è«‹**å¿½ç•¥**è¡Œé¦–çš„å¸¸è¦‹è¨»è§£ç¬¦è™Ÿï¼ˆå¦‚ `//`, `/*`, `*`, `#`ï¼‰ã€‚
3. **é‡é»**ï¼šæˆ‘å€‘åªæª¢æŸ¥ã€Œæœ‰æ•ˆçš„æ–‡å­—å…§å®¹ã€ã€‚

- åˆ¤å®šè¦å‰‡ï¼š
  - âŒ **FAIL (ç¼ºè¡Œ)**ï¼šç¨‹å¼ç¢¼ä¸­å®Œå…¨æ‰¾ä¸åˆ°æ­¤è¡Œï¼Œå›å ±ã€Œæœªå‡ºç¾åœ¨ç¨‹å¼ç¢¼ä¸­ã€ã€‚
  - âœ… **PASS**ï¼šè©²è¡Œå­˜åœ¨ï¼Œä¸”å†’è™Ÿå¾Œæœ‰å…·é«”çš„æˆæ¬Šåç¨± (ä¾‹å¦‚ `Apache-2.0` æˆ– `GPL-3.0`)ã€‚

- åˆ¤å®šè¦å‰‡ï¼š
  - âŒ **FAIL (ç¼ºè¡Œ)**ï¼šç¨‹å¼ç¢¼ä¸­å®Œå…¨æ‰¾ä¸åˆ°æ­¤è¡Œï¼Œå›å ±ã€Œæœªå‡ºç¾åœ¨ç¨‹å¼ç¢¼ä¸­ã€ã€‚
  - âŒ **FAIL (å…¬å¸åç¨±éŒ¯èª¤)**ï¼šå¿…é ˆé€å­—ç²¾ç¢ºåŒ¹é… `RoyalTek Co., Ltd.`ã€‚
    - æ³¨æ„å¤§å°å¯«ï¼š`RoyalTek` (T å¿…é ˆå¤§å¯«)ã€‚
    - æ³¨æ„æ¨™é»ï¼š`Co., Ltd.` (å¿…é ˆæœ‰é€—è™Ÿèˆ‡å¥é»)ã€‚
  - âŒ **FAIL (å¹´ä»½é‚è¼¯éŒ¯èª¤)**ï¼š
    - è‹¥ç‚ºå–®ä¸€å¹´ä»½ (e.g., `2025`)ï¼šå¿…é ˆ <= {current_year}ã€‚
    - è‹¥ç‚ºå¹´ä»½ç¯„åœ (e.g., `2020-2025`)ï¼š**çµæŸå¹´ä»½** å¿…é ˆ <= {current_year}ã€‚
    - è‹¥å¹´ä»½å¤§æ–¼ {current_year} (æœªä¾†æ™‚é–“)ï¼Œè¦–ç‚º FAILã€‚
  - âœ… **PASS**ï¼šè©²è¡Œå­˜åœ¨ï¼Œä¸”å…¬å¸åç¨± (RoyalTek Co., Ltd.) å®Œå…¨æ­£ç¢ºï¼Œå¹´ä»½äº¦ç¬¦åˆé‚è¼¯ã€‚

## 3. Author
- æ¨™æº–æ ¼å¼ï¼š`Author: Name <Email>`
- åˆ¤å®šè¦å‰‡ï¼š
  - âŒ **FAIL (ç¼ºè¡Œ)**ï¼šç¨‹å¼ç¢¼ä¸­å®Œå…¨æ‰¾ä¸åˆ°æ­¤è¡Œï¼Œå›å ±ã€Œæœªå‡ºç¾åœ¨ç¨‹å¼ç¢¼ä¸­ã€ã€‚
  - âŒ **FAIL (æ ¼å¼éŒ¯èª¤)**ï¼šEmail **å¿…é ˆ**è¢«è§’æ‹¬è™Ÿ `< >` åŒ…åœï¼Œä½† Name (å§“å) **ä¸å¯**åŒ…åœã€‚
    - éŒ¯èª¤ç¯„ä¾‹ï¼š`Author: KJ Chang (KJ.Chang@royaltek.com)` (Email ç”¨äº†åœ“æ‹¬è™Ÿ)ã€‚
    - éŒ¯èª¤ç¯„ä¾‹ï¼š`Author: <KJ Chang> <KJ.Chang@royaltek.com>` (Name ä¸è©²æœ‰è§’æ‹¬è™Ÿ)ã€‚
    - éŒ¯èª¤ç¯„ä¾‹ï¼š`Author: KJ Chang KJ.Chang@royaltek.com` (Email æ¼äº†è§’æ‹¬è™Ÿ)ã€‚
  - âŒ **FAIL (ç¶²åŸŸéŒ¯èª¤)**ï¼šEmail å¿…é ˆä»¥ `@royaltek.com` çµå°¾ã€‚
  - âœ… **PASS**ï¼šè©²è¡Œå­˜åœ¨ï¼Œæ ¼å¼ç¬¦åˆ `Name <Email>` (Name ç‚ºç´”æ–‡å­—ï¼ŒEmail å«è§’æ‹¬è™Ÿ)ï¼Œä¸” Email ä»¥ `@royaltek.com` çµå°¾ã€‚

---

# PART 2: é¢¨éšªèˆ‡å“è³ªè¦ç¯„ (Risk List)

**âš ï¸ ID å¼•ç”¨è¦å‰‡ (IMPORTANT)**ï¼š
- **Risk ID ä¾†æº**ï¼šè«‹**ç›´æ¥è¤‡è£½**è¦å‰‡åˆ—è¡¨é–‹é ­çš„**æ–¹æ‹¬è™Ÿæ¨™ç±¤** (ä¾‹å¦‚ `[CRITICAL-1]` æˆ– `[LOW-2]`)ã€‚
- **çµ•å°ç¦æ­¢é‡æ–°ç·¨è™Ÿ**ï¼šä¸ç®¡é€™æ˜¯ä½ ç™¼ç¾çš„ç¬¬å¹¾å€‹å•é¡Œï¼ŒID å¿…é ˆå®Œå…¨ä¾ç…§è©²è¦å‰‡åœ¨åˆ—è¡¨ä¸­çš„æ¨™ç±¤ã€‚
- **å…è¨±ä¸¦è¦æ±‚é‡è¤‡å¼•ç”¨**ï¼šå¦‚æœåŒä¸€ç¨®é¢¨éšªåœ¨ä¸åŒè¡Œæ•¸å‡ºç¾å¤šæ¬¡ï¼ˆä¾‹å¦‚å¤šå€‹åœ°æ–¹éƒ½æœ‰ Magic Numberï¼‰ï¼Œ**å¿…é ˆåˆ†é–‹åˆ—å‡º**ã€‚
    - âœ… **æ­£ç¢º**ï¼šåˆ—å‡ºå…©å€‹ç¨ç«‹çš„å€å¡Šï¼Œéƒ½ä½¿ç”¨ `Risk ID: [MEDIUM-2]`ï¼Œä½† `Location` ä¸åŒ (ä¾‹å¦‚ä¸€å€‹æ˜¯ `0010`ï¼Œå¦ä¸€å€‹æ˜¯ `0055`)ã€‚
    - âŒ **éŒ¯èª¤**ï¼šå°‡æ‰€æœ‰è¡Œè™Ÿåˆä½µåœ¨åŒä¸€å€‹å€å¡Šä¸­ï¼ˆä¾‹å¦‚ `Location: 0010, 0055, 0092`ï¼‰ã€‚
- **ç¯„ä¾‹**ï¼š
    - è‹¥è¦å‰‡å¯«è‘— `[LOW-2] Naming...`ï¼Œä½ çš„å ±å‘Šä¸­å¿…é ˆå¯« `Risk ID: [LOW-2]`ã€‚
    - **éŒ¯èª¤ç¯„ä¾‹**ï¼š`Risk ID: 2` (æœªåŒ…å«å®Œæ•´æ¨™ç±¤) æˆ– `Risk ID: LOW-02` (è‡ªä½œè°æ˜è£œé›¶)ã€‚

è«‹æ ¹æ“šä»¥ä¸‹åˆ—è¡¨æƒæç¨‹å¼ç¢¼å•é¡Œ (è‹¥ç„¡å•é¡Œå‰‡ä¸éœ€åˆ—å‡º)ï¼š

**âš ï¸ é‡è¦æŒ‡ä»¤ (IMPORTANT)**ï¼š
- **å¿…é ˆæª¢æŸ¥å®Œæ•´ç¨‹å¼ç¢¼**ï¼šç„¡è«–ç¨‹å¼ç¢¼å¤šé•·ï¼Œè«‹å‹™å¿…å¾ç¬¬ä¸€è¡Œæª¢æŸ¥åˆ°æœ€å¾Œä¸€è¡Œã€‚
- **ä¸å¯çœç•¥**ï¼šä¸è¦å› ç‚ºç¨‹å¼ç¢¼éé•·è€Œåœæ­¢æª¢æŸ¥æˆ–åªæª¢æŸ¥éƒ¨åˆ†ç‰‡æ®µã€‚
- **ä¸å¯æ‘˜è¦**ï¼šè«‹åˆ—å‡ºæ‰€æœ‰ç™¼ç¾çš„å•é¡Œï¼Œä¸è¦åªåˆ—å‡ºå‰å¹¾å€‹ã€‚
- **Location æ ¼å¼**ï¼šè«‹å‹™å¿…ä½¿ç”¨ Input Code å·¦å´é¡¯ç¤ºçš„ **4 ç¢¼æ•¸å­—** (ä¾‹å¦‚ `0005`, `0120`)ï¼Œä¸è¦è‡ªè¡Œç°¡åŒ–ç‚º `5` æˆ– `120`ã€‚

## ğŸ”´ [CRITICAL] (Must be fixed immediately. Causes crashes, security vulnerabilities, or hardware damage.)
[CRITICAL-1]  Memory Management: Check return values of `malloc` or `new`. Log errors and handle failures.
[CRITICAL-2]  File Operations: Check return values for file open/read/write. Log errors and notify upper layers.
[CRITICAL-3]  Concurrency - Race Conditions: Use locks or mutexes to protect shared resources.
[CRITICAL-4]  Concurrency - Deadlocks: Ensure consistent lock acquisition order and set maximum wait times.
[CRITICAL-5]  Concurrency - Thread Safety: Only use thread-safe APIs in multi-threaded environments.
[CRITICAL-6]  Loops: All loops (`while`, `for`, `goto`) must have clear entry and exit conditions to avoid infinite loops.
[CRITICAL-7]  Security - Unsafe Functions: Avoid unsafe functions like `strcpy` (C++) or SQL injection vulnerabilities (Java).
[CRITICAL-8]  Security - Encryption: Passwords and keys must be stored and transmitted encrypted.
[CRITICAL-9]  Security - Hard-coding: Do not hard-code sensitive data in source code or config files.
[CRITICAL-10] Security - Input Validation: Verify and filter all user input data.
[CRITICAL-11] Security - Measures: Ensure necessary measures to prevent data theft or tampering.
[CRITICAL-12] Stability: Software must not crash.
[CRITICAL-13] Android/UI: Ensure UI updates are ONLY performed in the UI thread.

## ğŸŸ  [HIGH] (Potential errors, performance bottlenecks, or system instability. Strongly recommended to fix.)
[HIGH-1]  Global Variables: Avoid them. Use local variables with read/write methods.
[HIGH-2]  Error Handling - Communication: Handle timeouts/disconnections with a retry mechanism (e.g., 3 retries).
[HIGH-3]  Error Handling - Exceptions: Use `try-catch` appropriately (Java/C++) and ensure resource release.
[HIGH-4]  Initialization: Initialize all variables. Check array/memory ranges before use.
[HIGH-5]  Resource Management: Ensure files, network connections, etc., are closed after use.
[HIGH-6]  Input Checks: All functions must perform type and value checks on input data.
[HIGH-7]  Security - Logging: Log security events (errors, exceptions) with context, excluding sensitive info.
[HIGH-8]  Performance - UI: Move time-consuming tasks to background threads.
[HIGH-9]  Performance - Resources: Release GUI resources in a timely manner.
[HIGH-10] Embedded - Recursion: Disallow recursive functions (stack overflow risk).
[HIGH-11] Protocol - UART: Use checksums to ensure data completeness.
[HIGH-12] Storage - eMMC: Avoid continuous writing; write to RAM and flush on shutdown.
[HIGH-13] Quality: Fix major bugs found by static analysis tools.

## ğŸŸ¡ [MEDIUM] (Improves maintainability, readability, or best practices.)
[MEDIUM-1]  Comments: Comment output, input, and key logic (`//` or `/**/`).
[MEDIUM-2]  Magic Numbers: Use meaningful variables/constants instead of hard-coded numbers.
[MEDIUM-3]  Complexity: Keep nested control structures within 3 layers. Refactor if deeper.
[MEDIUM-4]  Control Flow: `if` and `switch` statements must consider `default` or exceptions.
[MEDIUM-5]  Android: Selectively disable Activity rebuild or use ViewModel.
[MEDIUM-6]  Audio: Check quality (echo cancellation, noise reduction).
[MEDIUM-7]  Testing: Unit and integration tests to ensure correctness.
[MEDIUM-8]  Process: Code review ensures quality.

## ğŸŸ¢ [LOW] (Style, formatting, and minor details.)
[LOW-1]  Naming: Class/Variables (Nouns), Functions (Verbs), CamelCase.
[LOW-2]  Naming: Constants in UPPERCASE_WITH_UNDERSCORES.
[LOW-3]  Structure: Separate `*.h`, `*.cpp`, `*.java` files per class.
[LOW-4]  UI Layout: Use appropriate layout managers, test multiple screen sizes.
[LOW-5]  UI Layout: Manage vertical and horizontal layouts.

---

# è¼¸å‡ºç¯„æœ¬ (Output Template)

è«‹**åš´æ ¼éµå®ˆ**ä»¥ä¸‹ç´”æ–‡å­—æ ¼å¼å›è¦†ï¼Œä¸è¦æ›´å‹•çµæ§‹ï¼š

==================================================
CODE REVIEW REPORT: {filename}
==================================================

>>> PART 1: HEADER CHECK RESULTS

    Status: [ âœ… PASS / âŒ FAIL ]
    Found : (è«‹è¤‡è£½ç¨‹å¼ç¢¼ä¸­çš„å¯¦éš›æ–‡å­—ï¼Œè¨˜å¾—å»é™¤è¡Œè™Ÿå‰ç¶´ï¼Œè‹¥ç„¡å‰‡å¯« "None")
    Reason: (è«‹èªªæ˜ç†ç”±)

    Status: [ âœ… PASS / âŒ FAIL ]
    Found : (è«‹è¤‡è£½ç¨‹å¼ç¢¼ä¸­çš„å¯¦éš›æ–‡å­—ï¼Œè¨˜å¾—å»é™¤è¡Œè™Ÿå‰ç¶´)
    Reason: (æª¢æŸ¥å¹´ä»½æ˜¯å¦ <= {current_year} ä¸”æ‹¼å­—å®Œå…¨æ­£ç¢º)

[3] Author
    Status: [ âœ… PASS / âŒ FAIL ]
    Found : (è«‹è¤‡è£½ç¨‹å¼ç¢¼ä¸­çš„å¯¦éš›æ–‡å­—ï¼Œè¨˜å¾—å»é™¤è¡Œè™Ÿå‰ç¶´)
    Reason: (æª¢æŸ¥æ˜¯å¦ç‚º @royaltek.com)

--------------------------------------------------

>>> PART 2: RISK ANALYSIS

(è‹¥æœªç™¼ç¾ä»»ä½•å•é¡Œï¼Œè«‹è¼¸å‡ºï¼š "âœ… No risks found.")

(é‡å°æ¯å€‹ç™¼ç¾çš„å•é¡Œï¼Œè«‹é‡è¤‡ä»¥ä¸‹å€å¡Š)
**âš ï¸ è­¦å‘Šï¼šè‹¥ç„¡ç™¼ç¾å…·é«”å•é¡Œï¼Œçµ•å°ä¸è¦è¼¸å‡ºæ­¤å€å¡Š (ä¸è¦å¡«å¯« None)**
[SEVERITY: (è«‹å¡«å¯«ç­‰ç´šï¼Œä¾‹å¦‚ ğŸ”´ CRITICAL)]
    Risk ID : (è«‹å¡«å¯«è¦å‰‡æ–¹æ‹¬è™Ÿä¸­çš„å®Œæ•´æ¨™ç±¤ï¼Œä¾‹å¦‚ [LOW-2])
    Location: (è«‹å¡«å¯« Input Code å·¦å´çš„ 4 ç¢¼è¡Œè™Ÿï¼Œä¾‹å¦‚ 0102)
    Problem : (å•é¡Œæè¿°)
    Fix     : (å…·é«”ä¿®æ­£å»ºè­°)
    -------------------------------------------

==================================================
END OF REPORT
==================================================
"""

    # Add line numbers to the code
    lines = file_code.splitlines()
    numbered_code = '\n'.join([f"{i+1:04d} | {line}" for i, line in enumerate(lines)])

    code_section = f"""
# Input Code
(The code content begins below)
â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“
{numbered_code}
â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘
(End of code content)
"""

    bottom_instructions = f"""
# Final Execution Instructions

ä½ å·²ç¶“é–±è®€å®Œæª”æ¡ˆ "{filename}" çš„æ‰€æœ‰ç¨‹å¼ç¢¼ã€‚ç¾åœ¨è«‹é–‹å§‹åŸ·è¡Œå¯©æŸ¥ï¼š

1.  **å›é ­æª¢æŸ¥ Header**ï¼šè«‹é‡æ–°æª¢è¦–ç¨‹å¼ç¢¼æœ€ä¸Šæ–¹çš„è¨»è§£ï¼Œæ¯”å° PART 1 è¦å‰‡ã€‚
    -   **ğŸ¯ å»é™¤è¡Œè™Ÿè¦å‰‡**ï¼šç•¶ä½ å¼•ç”¨ç¨‹å¼ç¢¼åˆ°ã€ŒFoundã€æ¬„ä½æ™‚ï¼Œå¿…é ˆåˆ‡é™¤è¡Œé¦–çš„ã€Œæ•¸å­— + åˆ†éš”ç·šã€ï¼Œä¿ç•™åŸå§‹è¨»è§£å…§å®¹ã€‚

2.  **æƒæé‚è¼¯é¢¨éšª**ï¼šè«‹é‡æ–°æª¢è¦–ç¨‹å¼ç¢¼é‚è¼¯ï¼Œæ¯”å° PART 2 é¢¨éšªåˆ—è¡¨ã€‚
    -   **Risk ID**ï¼šè«‹ç›´æ¥è¤‡è£½è¦å‰‡æ–‡å­—é–‹é ­çš„æ–¹æ‹¬è™Ÿæ¨™ç±¤ (ä¾‹å¦‚ `[MEDIUM-2]`)ã€‚
        -   âœ… æ­£ç¢ºç¯„ä¾‹ï¼š`Risk ID : [HIGH-5]`
        -   âŒ éŒ¯èª¤ç¯„ä¾‹ï¼š`Risk ID : 5` (éºæ¼å‰ç¶´) æˆ– `Risk ID : [HIGH-05]` (è‡ªä½œè°æ˜è£œé›¶)
    -   **Location**ï¼šè«‹å‹™å¿…å¡«å¯« **Input Code å·¦å´é¡¯ç¤ºçš„ 4 ç¢¼è¡Œè™Ÿ** (ä¾‹å¦‚ `0444`)ï¼Œ**çµ•å°ä¸è¦**è‡ªå·±ç™¼æ˜è¡Œè™Ÿã€‚
        -   âœ… æ­£ç¢ºç¯„ä¾‹ï¼š`Location : 0444`
        -   âŒ éŒ¯èª¤ç¯„ä¾‹ï¼š`Location : 444` (éºæ¼å‰ç¶´)

3.  **å†æ¬¡ç¢ºèªå®Œæ•´æ€§**ï¼š
    -   è«‹ç¢ºèªæ‚¨å·²ç¶“æª¢æŸ¥äº†**æ¯ä¸€è¡Œç¨‹å¼ç¢¼**ï¼Œæ²’æœ‰éºæ¼ä»»ä½•éƒ¨åˆ†ã€‚
    -   å¦‚æœç¨‹å¼ç¢¼å¾ˆé•·ï¼Œè«‹ç¢ºä¿æ‚¨æ²’æœ‰å› ç‚ºé•·åº¦è€Œå¿½ç•¥äº†å¾Œé¢çš„éƒ¨åˆ†ã€‚

4.  **ç”¢ç”Ÿå ±å‘Š**ï¼š
-   **ç¦æ­¢ç©ºå€å¡Š**ï¼šåœ¨è¼¸å‡ºæ¯ä¸€å€‹ Risk å€å¡Šå‰ï¼Œè«‹å…ˆç¢ºèª `Problem` å’Œ `Fix` æ˜¯å¦æœ‰å¯¦è³ªå…§å®¹ã€‚å¦‚æœåªæ˜¯æƒ³å¡« `None`ï¼Œè«‹ç›´æ¥è·³éè©²å€å¡Šï¼Œ**çµ•å°ä¸è¦è¼¸å‡º**ã€‚
    -   **ç¦æ­¢ Markdown è¡¨æ ¼** (ä¸è¦ç”¨ `|`)ã€‚
    -   **ä½¿ç”¨ç´”æ–‡å­—æ ¼å¼** (ä¾ç…§ä¸Šæ–¹çš„ "Output Template")ã€‚
    -   **èªè¨€**ï¼šè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ (Traditional Chinese) æ’°å¯«å ±å‘Šå…§å®¹ã€‚

**Action**: è«‹ä¾ç…§ç¯„æœ¬è¼¸å‡º "CODE REVIEW REPORT"ï¼š
"""

    prompt = f"{top_instructions}\n{code_section}\n{bottom_instructions}"
    return prompt.strip()

def call_ollama_api(prompt):
    try:
        if OLLAMA_MODE == "chat":
            url = f"{OLLAMA_HOST}/api/chat"
            payload = {
                "model": MODEL,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False
            }
        elif OLLAMA_MODE == "generate":
            url = f"{OLLAMA_HOST}/api/generate"
            payload = {
                "model": MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_ctx": 65536,  # ç¨‹å¼ç¢¼è¡Œæ•¸è¦æ›´å¤šè¦å†å¢åŠ ï¼Œç›®å‰å¤§ç´„å¯ä»¥åˆ°6500è¡Œ
                    "num_predict": -1, 
                    "temperature": 0,
                    "top_k": 20, 
                    "top_p": 0.9, 
                    "repeat_penalty": 1.05, 
                    "seed": 42
                }
            }
        else:
            return f"[âŒ Invalid OLLAMA_MODE: {OLLAMA_MODE} (should be 'chat' or 'generate')]"

        response = requests.post(
            url,
            headers={"Content-Type": "application/json"},
            json=payload,
            timeout=600
        )
        response.raise_for_status()

        if OLLAMA_MODE == "chat":
            return response.json().get("message", {}).get("content", "[âš ï¸ No chat response from Ollama]")
        else:
            return response.json().get("response", "[âš ï¸ No generate response from Ollama]")

    except Exception as e:
        return f"[âŒ Unable to connect to Ollama ({OLLAMA_MODE}): {e}]"

def call_hf_api(prompt):
    try:
        response = requests.post(
            HF_API_URL,
            headers={"Content-Type": "application/json"},
            json={
                "model": MODEL,
                "prompt": prompt,
                "stream": False
            },
            timeout=120
        )
        response.raise_for_status()
        return response.json().get("response", "[âš ï¸ No response content from HF API]")
    except Exception as e:
        return f"[âŒ Unable to connect to HF API: {e}]"

def call_model_api(prompt):
    if BACKEND == "ollama":
        return call_ollama_api(prompt)
    elif BACKEND == "hf":
        return call_hf_api(prompt)
    else:
        return "[âŒ Invalid backend specified. Use 'ollama' or 'hf']"


def parse_review_result(ai_response):
    """
    è§£æ AI å›è¦†ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰ Header é•è¦æˆ–é¢¨éšªã€‚
    å›å‚³ä¸€å€‹å­—å…¸ï¼ŒåŒ…å«æª¢æŸ¥çµæœã€‚
    """
    result = {
        "header_fail": False,
        "risks": []
    }

    # 1. æª¢æŸ¥ Header (Part 1)
    # åŒæ™‚æ”¯æ´æœ‰åœ–ç¤ºæˆ–ç„¡åœ–ç¤ºçš„å¯«æ³•
    if re.search(r"Status:.*(FAIL|âŒ)", ai_response, re.IGNORECASE):
        result["header_fail"] = True

    # 2. æª¢æŸ¥ Risks (Part 2)
    # å¿½ç•¥å‰é¢çš„åœ–ç¤ºï¼ŒåªæŠ“å–æ–‡å­—ç­‰ç´š
    risk_pattern = re.compile(r"\[SEVERITY:.*(CRITICAL|HIGH)\]")
    result["risks"] = risk_pattern.findall(ai_response)

    return result


def post_comment_to_merge_request(message):
    mr_id = os.getenv("CI_MERGE_REQUEST_IID")
    project_id = os.getenv("CI_PROJECT_ID")
    api_base = os.getenv("CI_API_V4_URL", "https://gitlab.com/api/v4")
    token = os.getenv("GITLAB_TOKEN")

    print("ğŸ” [DEBUG] MR ID:", mr_id)
    print("ğŸ” [DEBUG] Project ID:", project_id)
    print("ğŸ” [DEBUG] API URL:", api_base)
    print("ğŸ” [DEBUG] Token Present:", "Yes" if token else "No")

    if not mr_id or not project_id or not token:
        print("âš ï¸ Unable to comment: Missing CI_MERGE_REQUEST_IID, CI_PROJECT_ID, or GITLAB_TOKEN")
        return

    url = f"{api_base}/projects/{project_id}/merge_requests/{mr_id}/notes"
    headers = {"PRIVATE-TOKEN": token}
    data = {"body": message}

    try:
        resp = requests.post(url, headers=headers, data=data)
        if resp.status_code == 201:
            print("âœ… Comment posted to Merge Request")
        else:
            print(f"âŒ Failed to post commentï¼š{resp.status_code} - {resp.text}")
    except Exception as e:
        print(f"âŒ Unable to submit commentï¼š{e}")


def main():
    print("ğŸ”  Starting AI code review...")
    changed_files = get_changed_files()
    
    if not changed_files:
        print("â„¹ï¸  No changed files detected. Skipping analysis.")    
        post_comment_to_merge_request(
        "ğŸ¤– AI Code Reviewï¼šNo changed files detected. Skipping analysis."
        )
        return

    has_violation = False

    for filepath in changed_files:
        print(f"\nğŸ“‚ Analyzingï¼š{filepath}")
        file_code = extract_code_from_file(filepath)
        prompt = generate_code_review_prompt(file_code, filepath, CURRENT_YEAR)
        ai_response = call_model_api(prompt)

        print("ğŸ¤– AI Suggestionsï¼š")
        print(ai_response)
        print("\n" + "=" * 80)

        comment_header = f"### ğŸ¤– AI Code Review Reportï¼š`{filepath}`"
        comment_body = f"{comment_header}\n\n```\n{ai_response}\n```"
        post_comment_to_merge_request(comment_body)

        # è§£æçµæœ
        analysis_result = parse_review_result(ai_response)

        # å¦‚æœ Header å¤±æ•— æˆ– Risks åˆ—è¡¨æœ‰å…§å®¹ï¼Œå°±æ¨™è¨˜ç‚ºé•è¦
        if analysis_result["header_fail"] or analysis_result["risks"]:
            has_violation = True

    if has_violation:
        print("âŒ Found violations")
        if STRICT_AI_REVIEW:
            print("ğŸš« Strict mode enabled. CI task marked as failed.")
            exit(1)
        else:
            print("âš ï¸ Non-strict mode. Suggestions provided but CI will not be blocked.")
    else:
        print("âœ… All changed files comply with company policies.")


if __name__ == "__main__":
    main()

# --- END OF ai_review.py ---

"""
LLM Chat Application
CLI chat interface using HuggingFace Transformers with openai/gpt-oss-120b
Optimized for NVIDIA DGX Spark (GB10)

See llmchat.md for full specification.
"""

import os
import sys
import re
import yaml
import logging
import threading
import warnings
from datetime import datetime
from pathlib import Path

import torch
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer

# Suppress the attention mask warning
warnings.filterwarnings("ignore", message=".*attention mask.*")
logging.getLogger("transformers").setLevel(logging.ERROR)


def load_config(config_path: str = "llmchat_config.yaml") -> dict:
    """Load configuration from YAML file."""
    default_config = {
        "model": "openai/gpt-oss-120b",
        "system_prompt": "You are a helpful AI assistant.",
        "reasoning_level": "medium",
        "max_new_tokens": 32768,
        "max_history_tokens": None,
        "show_thinking": False,  # Show chain-of-thought reasoning
    }
    
    config_file = Path(config_path)
    if config_file.exists():
        with open(config_file, "r") as f:
            user_config = yaml.safe_load(f) or {}
            default_config.update(user_config)
    else:
        print(f"[INFO] Config file not found at {config_path}, using defaults.")
    
    return default_config


def build_system_prompt(base_prompt: str, reasoning_level: str) -> str:
    """Build system prompt with reasoning level."""
    return f"{base_prompt}\n\nReasoning: {reasoning_level}"


class LLMChat:
    """Main chat application class."""
    
    def __init__(self, config: dict):
        self.config = config
        self.model_id = config["model"]
        self.system_prompt = config["system_prompt"]
        self.reasoning_level = config["reasoning_level"]
        self.max_new_tokens = config["max_new_tokens"]
        self.max_history_tokens = config["max_history_tokens"]
        self.show_thinking = config.get("show_thinking", False)
        
        self.conversation_history = []
        self.model = None
        self.tokenizer = None
        self.streamer = None
    
    def parse_response(self, response: str) -> tuple:
        """
        Parse gpt-oss response to separate thinking from final answer.
        Returns (thinking, final_answer).
        """
        # Common patterns for gpt-oss chain-of-thought
        # Pattern: ...thinking...assistantfinal<answer>
        patterns = [
            r'(.*)assistantfinal(.*)$',
            r'(.*)\[FINAL\](.*)$',
            r'(.*)\n---\n(.*)$',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
            if match:
                thinking = match.group(1).strip()
                final = match.group(2).strip()
                return thinking, final
        
        # No pattern matched, return as-is
        return "", response.strip()
        
    def load_model(self):
        """Load the model and tokenizer."""
        print(f"[INFO] Loading model: {self.model_id}")
        print("[INFO] This may take a few minutes...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        
        # Load model - try different strategies to avoid meta device issues
        try:
            # Strategy 1: Use device_map with offload disabled
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                torch_dtype=torch.bfloat16,
                device_map="cuda:0",  # Explicit single device instead of "auto"
                trust_remote_code=True,
            )
        except Exception as e:
            print(f"[WARN] Strategy 1 failed: {e}")
            print("[INFO] Trying alternative loading strategy...")
            # Strategy 2: Load without device_map, then move to GPU
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
            )
            self.model = self.model.cuda()
        
        self.streamer = TextIteratorStreamer(
            self.tokenizer,
            skip_prompt=True,
            skip_special_tokens=True,
        )
        
        print("[INFO] Model loaded successfully!")
        print(f"[INFO] Reasoning level: {self.reasoning_level}")
        print(f"[INFO] Max tokens: {self.max_new_tokens}")
        print()
    
    def get_full_system_prompt(self) -> str:
        """Get system prompt with reasoning level."""
        return build_system_prompt(self.system_prompt, self.reasoning_level)
    
    def build_messages(self, user_input: str) -> list:
        """Build messages list for the model."""
        messages = [
            {"role": "system", "content": self.get_full_system_prompt()},
        ]
        messages.extend(self.conversation_history)
        messages.append({"role": "user", "content": user_input})
        return messages
    
    def generate_response(self, user_input: str):
        """Generate streaming response."""
        messages = self.build_messages(user_input)
        
        # Apply chat template
        inputs = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
        ).to(self.model.device)
        
        # Generation kwargs
        generation_kwargs = {
            "input_ids": inputs,
            "max_new_tokens": self.max_new_tokens,
            "streamer": self.streamer,
            "do_sample": True,
            "temperature": 0.7,
            "top_p": 0.9,
            "pad_token_id": self.tokenizer.eos_token_id,
        }
        
        # Run generation in a thread
        thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()
        
        # Collect full response (stream to buffer, not screen)
        full_response = ""
        if self.show_thinking:
            # Show everything as it streams
            print("Assistant: ", end="", flush=True)
            for token in self.streamer:
                print(token, end="", flush=True)
                full_response += token
            print()
        else:
            # Collect silently, then parse
            print("[Thinking...]", end="", flush=True)
            for token in self.streamer:
                full_response += token
            print("\r" + " " * 20 + "\r", end="")  # Clear "Thinking..."
        
        thread.join()
        
        # Parse response to separate thinking from final answer
        thinking, final_answer = self.parse_response(full_response)
        
        if not self.show_thinking:
            print(f"Assistant: {final_answer}")
            if thinking:
                # Store thinking for debugging but don't display
                pass
        
        # Update conversation history (store full response for context)
        self.conversation_history.append({"role": "user", "content": user_input})
        self.conversation_history.append({"role": "assistant", "content": full_response})
        
        return final_answer
    
    def handle_command(self, command: str) -> bool:
        """
        Handle special commands.
        Returns True if should continue, False if should exit.
        """
        parts = command.strip().split(maxsplit=1)
        cmd = parts[0].lower()
        arg = parts[1] if len(parts) > 1 else ""
        
        if cmd == "/bye":
            print("[INFO] Goodbye!")
            return False
        
        elif cmd == "/clear":
            self.conversation_history = []
            print("[INFO] Conversation history cleared.")
            return True
        
        elif cmd == "/system":
            if arg:
                self.system_prompt = arg
                print(f"[INFO] System prompt updated to: {arg}")
            else:
                print(f"[INFO] Current system prompt: {self.system_prompt}")
            return True
        
        elif cmd == "/save":
            filename = arg if arg else f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            self.save_conversation(filename)
            return True
        
        elif cmd == "/reason":
            if arg.lower() in ["low", "medium", "high"]:
                self.reasoning_level = arg.lower()
                print(f"[INFO] Reasoning level set to: {self.reasoning_level}")
            else:
                print(f"[INFO] Current reasoning level: {self.reasoning_level}")
                print("[INFO] Valid levels: low, medium, high")
            return True
        
        else:
            print(f"[WARN] Unknown command: {cmd}")
            print("[INFO] Available commands: /bye, /clear, /system, /save, /reason")
            return True
    
    def save_conversation(self, filename: str):
        """Save conversation history to file."""
        with open(filename, "w") as f:
            f.write(f"# LLM Chat Conversation\n")
            f.write(f"# Model: {self.model_id}\n")
            f.write(f"# Date: {datetime.now().isoformat()}\n")
            f.write(f"# System Prompt: {self.system_prompt}\n")
            f.write(f"# Reasoning Level: {self.reasoning_level}\n\n")
            
            for msg in self.conversation_history:
                role = msg["role"].upper()
                content = msg["content"]
                f.write(f"[{role}]\n{content}\n\n")
        
        print(f"[INFO] Conversation saved to: {filename}")
    
    def run(self):
        """Main chat loop."""
        import sys
        
        # Reconfigure stdin for UTF-8
        if hasattr(sys.stdin, 'reconfigure'):
            sys.stdin.reconfigure(encoding='utf-8', errors='replace')
        
        self.load_model()
        
        print("=" * 60)
        print("LLM Chat - Type '/bye' to exit, '/help' for commands")
        print("=" * 60)
        print()
        
        while True:
            try:
                print("You: ", end="", flush=True)
                user_input = sys.stdin.readline()
                if not user_input:  # EOF
                    print("\n[INFO] EOF received. Goodbye!")
                    break
                user_input = user_input.strip()
                
                if not user_input:
                    continue
                
                # Handle commands
                if user_input.startswith("/"):
                    if not self.handle_command(user_input):
                        break
                    continue
                
                # Generate response
                self.generate_response(user_input)
                print()
                
            except KeyboardInterrupt:
                print("\n[INFO] Interrupted. Type '/bye' to exit or continue chatting.")
            except EOFError:
                print("\n[INFO] EOF received. Goodbye!")
                break


def main():
    """Main entry point."""
    # Check for custom config path
    config_path = "llmchat_config.yaml"
    if len(sys.argv) > 1:
        config_path = sys.argv[1]
    
    # Load config and start chat
    config = load_config(config_path)
    chat = LLMChat(config)
    chat.run()


if __name__ == "__main__":
    main()

# --- END OF llmchat.py ---

import torch
import sys

print(f"Python: {sys.version.split()[0]}")
print(f"PyTorch: {torch.__version__}")

if torch.cuda.is_available():
    print(f"\nSUCCESS: CUDA is available.")
    print(f"Device Name: {torch.cuda.get_device_name(0)}")
    print(f"Device Capability: {torch.cuda.get_device_capability(0)}")
    
    # Simple tensor test
    try:
        x = torch.tensor([1.0, 2.0]).cuda()
        print(f"Tensor on GPU: {x}")
        print("Basic GPU computation verification passed.")
    except Exception as e:
        print(f"FAILED to move tensor to GPU: {e}")
else:
    print("\nWARNING: CUDA is NOT available. PyTorch is using CPU.")
    print("Please ensure you have installed the correct version of PyTorch for your CUDA drivers.")


# --- END OF check_gpu.py ---

import torch
import transformers

print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")
    # Check if you have enough memory for the 120B model
    total_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    print(f"Total Unified Memory: {total_mem:.2f} GB")

    if total_mem < 80:
        print("âš ï¸ Warning: You might need heavy quantization for the 120B model.")
    else:
        print("âœ… Hardware looks great for large models!")
else:
    print("âŒ CUDA is not available. Check your installation.")

# --- END OF envcheck.py ---


# --- INJECTED BUG: SQL INJECTION ---
def get_user_logs(username):
    import sqlite3
    conn = sqlite3.connect('logs.db')
    cursor = conn.cursor()
    # Unsafe query construction
    query = "SELECT * FROM access_logs WHERE user = '" + username + "'"
    cursor.execute(query)
    return cursor.fetchall()

# --- INJECTED BUG: RESOURCE LEAK ---
def load_config_unsafe(path):
    f = open(path, 'r')
    return f.read()
    # File not closed
# --- REPEATED CONTENT FOR VOLUME ---
# --- INJECTED BUG: GLOBAL VARIABLE ---
GLOBAL_REQUEST_COUNTER = 0
def unsafe_increment():
    global GLOBAL_REQUEST_COUNTER
    # Race condition here
    temp = GLOBAL_REQUEST_COUNTER
    GLOBAL_REQUEST_COUNTER = temp + 1

"""
AI Code Reviewer
CLI tool using Qwen3-Coder-30B to review code files.
Optimized for NVIDIA DGX Spark (GB10).

See codereview.md for full specification.
"""

import os
import sys
import glob
from pathlib import Path
from typing import List, Optional

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Supported extensions for directory scanning
SUPPORTED_EXTENSIONS = {
    '.py', '.c', '.cpp', '.h', '.hpp', '.cc', 
    '.java', '.js', '.ts', '.go', '.rs', '.sh', 
    '.kt', '.swift'
}

# Model Config
MODEL_ID = "Qwen/Qwen3-Coder-30B-A3B-Instruct"
CHUNK_SIZE = 500  # Lines per review chunk

class CodeReviewer:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"

    def load_model(self):
        """Load the model and tokenizer."""
        print(f"[INFO] Loading model: {MODEL_ID}")
        print(f"[INFO] Target Device: {self.device}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True, local_files_only=True)
            
            # Using device_map="cuda:0" and bfloat16 as per spec
            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_ID,
                torch_dtype=torch.bfloat16,
                device_map=self.device,
                trust_remote_code=True,
                use_safetensors=True,
                low_cpu_mem_usage=True,
                attn_implementation="sdpa",
                local_files_only=True,
            )
            print("[INFO] Model loaded successfully!")
        except Exception as e:
            print(f"[ERROR] Failed to load model: {e}")
            sys.exit(1)

    def get_comment_style(self, file_extension: str) -> str:
        """Return the appropriate comment prefix for the file type."""
        ext = file_extension.lower()
        if ext in ['.py', '.sh', '.yaml', '.yml', '.rb']:
            return "# REVIEW: "
        elif ext in ['.c', '.cpp', '.h', '.hpp', '.cc', '.java', '.js', '.ts', '.go', '.rs', '.kt', '.swift']:
            return "// REVIEW: "
        else:
            return "REVIEW: " # Default fallback

    def review_chunk(self, content: str, start_line: int, end_line: int, comment_prefix: str, is_first_chunk: bool) -> str:
        """Generate review for a specific line range."""
        
        header_instruction = ""
        if is_first_chunk:
            header_instruction = (
                "CRITICAL INSTRUCTION: You MUST comment on missing headers.\n"
            )

        system_prompt = (
            "You are an expert Senior Software Engineer and Security Auditor.\n"
            "Your task is to review the provided source code and produce a UNIFIED DIFF that inserts comments where issues are found.\n"
            "\n"
            "OUTPUT INSTRUCTIONS:\n"
            "1. Output ONLY a Unified Diff (patch). Do NOT output the full source file.\n"
            "2. The diff should apply to the original file to add your comments.\n"
            f"3. Use the comment prefix '{comment_prefix}'.\n"
            "4. Insert comments immediately BEFORE the line they refer to.\n"
            "5. Use standard Unified Diff format:\n"
            "   --- original\n"
            "   +++ reviewed\n"
            "   @@ -line,count +line,count @@\n"
            "    context line\n"
            "   +comment line\n"
            "    target line\n"
            "\n"
            "6. Do not wrap the output in Markdown code blocks. Just output raw diff text.\n"
            "7. IGNORE existing comments in the code that look like issue tags (e.g. '[CRITICAL-1]').\n"
            "   You must generate your OWN review comments with the correct prefix.\n"
            "\n"
            f"FOCUS INSTRUCTION: The full file is provided for context, but you must ONLY review and output diffs for lines {start_line} to {end_line}.\n"
            "Do NOT output any diff hunks outside this range.\n"
            "\n"
            f"{header_instruction}\n"
            "REVIEW RULES:\n"
            "PART 1: HEADERS (MANDATORY CHECKS - Only for first chunk)\n"
            "- [HEADER-3] Check for Author information.\n"
            f"   -> IF MISSING: Insert a comment at line 1: '{comment_prefix}[HEADER-X] Missing header...'.\n"
            "\n"
            "PART 2: CRITICAL RISKS (Must Fix)\n"
            "- [CRITICAL-1] Memory: Check malloc/new return values and free/delete usage.\n"
            "- [CRITICAL-3/4/5] Concurrency: Check for race conditions, deadlocks, and thread safety.\n"
            "- [CRITICAL-7] Security: Avoid unsafe functions (strcpy, SQL injection).\n"
            "- [CRITICAL-9] Security: NO hard-coded secrets/passwords/keys.\n"
            "- [CRITICAL-10] Security: Validate all user inputs.\n"
            "\n"
            "PART 3: HIGH RISKS (Strongly Recommended)\n"
            "- [HIGH-1] Avoid Global Variables.\n"
            "- [HIGH-3] Error Handling: Use try-catch/result checks and ensure resource cleanup.\n"
            "- [HIGH-5] Resources: Close files, sockets, and connections.\n"
            "- [HIGH-7] Logging: Log security events but exclude sensitive info.\n"
            "\n"
            "PART 4: MEDIUM RISKS (Best Practices)\n"
            "- [MEDIUM-2] Avoid Magic Numbers -> Use constants.\n"
            "- [MEDIUM-3] Reduce Complexity -> Refactor deep nesting (>3 layers).\n"
            "- [MEDIUM-4] Control Flow -> Handle default/else cases.\n"
            "\n"
            "PART 5: LOW RISKS (Style)\n"
            "- [LOW-1] Naming: Enforce standard conventions. For Python, flag ONLY if function/variable names are NOT snake_case. For Java/C++, flag ONLY if NOT CamelCase.\n"
            "- [LOW-3] Structure: Keep classes in separate files where appropriate.\n"
        )
        
        # Add line numbers to content for the model to see
        # This helps the model respect the line range
        # Note: We don't change the actual content input, but we rely on the model counting.
        # Alternatively, we can prepend line numbers, but that might confuse the diff generation.
        # Qwen-Coder is good at counting, so we will try raw content first.
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Filename: input_file\n\n{content}"}
        ]

        # Prepare inputs
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)

        # Ensure pad_token is set
        if self.tokenizer.pad_token_id is None:
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        # Generate
        generated_ids = self.model.generate(
            model_inputs.input_ids,
            attention_mask=model_inputs.attention_mask,
            max_new_tokens=4096, # reduced token limit per chunk is fine
            temperature=0.2, 
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id
        )
        
        # Decode
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        return self._clean_response(response)

    def generate_review(self, file_path: Path) -> str:
        """Read file and generate review content in chunks."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"[WARN] Could not read {file_path}: {e}")
            return ""

        comment_prefix = self.get_comment_style(file_path.suffix)
        line_count = len(content.splitlines())
        
        print(f"    File has {line_count} lines. Analyzing in chunks of {CHUNK_SIZE} lines...")
        
        full_diff = ""
        
        for start_line in range(1, line_count + 1, CHUNK_SIZE):
            end_line = min(start_line + CHUNK_SIZE - 1, line_count)
            is_first = (start_line == 1)
            
            print(f"    -> Processing chunk: Lines {start_line}-{end_line}")
            
            chunk_diff = self.review_chunk(content, start_line, end_line, comment_prefix, is_first)
            
            if chunk_diff.strip():
                full_diff += f"\n{chunk_diff}\n"
                
        return full_diff.strip()

    def _clean_response(self, response: str) -> str:
        """Clean up markdown code blocks if the model included them."""
        lines = response.splitlines()
        
        # Check for start/end code fences
        if lines and lines[0].strip().startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].strip().startswith("```"):
            lines = lines[:-1]
            
        return "\n".join(lines)

    def process_path(self, path_str: str):
        """Process a file or directory."""
        path = Path(path_str)
        
        if not path.exists():
            print(f"[ERROR] Path does not exist: {path_str}")
            return

        files_to_process = []
        
        if path.is_file():
            files_to_process.append(path)
        elif path.is_dir():
            for ext in SUPPORTED_EXTENSIONS:
                # Recursive search for supported extensions
                files_to_process.extend(path.rglob(f"*{ext}"))
        
        if not files_to_process:
            print("[INFO] No supported source files found.")
            return

        print(f"[INFO] Found {len(files_to_process)} file(s) to review.")
        
        if self.model is None:
            self.load_model()

        for file_p in files_to_process:
            # Skip existing review files or diffs to avoid loops
            if file_p.name.endswith(".diff") or file_p.name.endswith("_r"):
                continue
            
            # Skip hidden files
            if any(part.startswith('.') for part in file_p.parts):
                continue

            print(f" -> Reviewing: {file_p}")
            reviewed_content = self.generate_review(file_p)
            
            if reviewed_content:
                output_path = file_p.parent / (file_p.name + ".diff")
                try:
                    with open(output_path, 'w', encoding='utf-8') as f:
                        f.write(reviewed_content)
                    print(f"    Saved to: {output_path}")
                except Exception as e:
                    print(f"    [ERROR] Writing file: {e}")

def main():
    if len(sys.argv) < 2:
        print("Usage: python codereview.py <file_or_folder>")
        sys.exit(1)
        
    target = sys.argv[1]
    reviewer = CodeReviewer()
    reviewer.process_path(target)

if __name__ == "__main__":
    main()

# --- END OF codereview.py ---


import os
import subprocess
import requests
import re
from datetime import datetime

BACKEND = os.getenv("BACKEND", "ollama")
STRICT_AI_REVIEW = os.getenv("STRICT_AI_REVIEW", "false").lower() == "true"
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://192.168.145.70:11434")
OLLAMA_MODE = os.getenv("OLLAMA_MODE", "generate").lower()  # chat or generate
HF_API_URL = os.getenv("HF_API_URL", "http://192.168.145.70:8000/generate")
TARGET_EXTENSIONS = [".py", ".c", ".cpp", ".h", ".hpp", ".cc", ".hh", ".kt", ".java"]
CURRENT_YEAR = datetime.now().year

if BACKEND == "hf":
    MODEL = os.getenv("HF_MODEL", "mistralai/Mistral-7B-Instruct-v0.1")
else:
    MODEL = os.getenv("OLLAMA_MODEL", "gpt-oss:120b")

def get_changed_files():
    """
    Compare the current HEAD with the target branch of the MR and retrieve the list of changed files
    """
    import json

    mr_iid = os.getenv("CI_MERGE_REQUEST_IID")
    project_id = os.getenv("CI_PROJECT_ID")
    api_base = os.getenv("CI_API_V4_URL", "https://gitlab.com/api/v4")
    token = os.getenv("GITLAB_TOKEN")

    if not mr_iid or not project_id or not token:
        print("âš ï¸ Unable to retrieve the MR target branch: missing CI_MERGE_REQUEST_IID / CI_PROJECT_ID / GITLAB_TOKEN")
        return []

    try:
        # Fetch Merge Request information
        url = f"{api_base}/projects/{project_id}/merge_requests/{mr_iid}"
        headers = {"PRIVATE-TOKEN": token}
        resp = requests.get(url, headers=headers)
        resp.raise_for_status()

        mr_data = resp.json()
        target_branch = mr_data.get("target_branch", "main")

        print(f"ğŸ”  MR #{mr_iid} target branch: {target_branch}")

        # Ensure the latest target branch data has been retrieved
        subprocess.run(["git", "fetch", "origin", target_branch], check=True)

        #  Compare differences
        result = subprocess.run(
            ["git", "diff", "--name-only", f"origin/{target_branch}...HEAD"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True,
            text=True
        )

        changed_files = result.stdout.strip().splitlines()

        if changed_files:
            print(f"âœ… Changed files detected vs origin/{target_branch}ï¼š{changed_files}")
        else:
            print(f"âš ï¸ No changed files detected vs origin/{target_branch}")

        return [
            f for f in changed_files
            if os.path.isfile(f)
            and f.endswith(tuple(TARGET_EXTENSIONS))
            and not f.startswith(".gitlab/")
        ]

    except Exception as e:
        print(f"âŒ Error during dynamic diff vs target branch: {e}")
        return []


def extract_code_from_file(file_path):
    """
    Extract code from file
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = []
            for line in f:
                lines.append(line)
            return ''.join(lines)
    except Exception as e:
        return f"[âŒ Unable to read file {file_path}: {e}]"

def generate_code_review_prompt(file_code, filename, current_year):
    """
    ç”ŸæˆåŒæ™‚åŒ…å« Header æª¢æŸ¥èˆ‡ç¨‹å¼ç¢¼å“è³ªå¯©æŸ¥çš„ Prompt (Sandwich Structure)
    """
    # --- INJECTED BUG: SECRET ---
    AWS_SECRET_KEY = 'AKIAIMNOVALIDKEY12345'

    top_instructions = f"""
# Role
ä½ æ˜¯ä¸€ä½**è³‡æ·±è»Ÿé«”æ¶æ§‹å¸«**èˆ‡**åš´æ ¼çš„åˆè¦ç¨½æ ¸å“¡**ã€‚
ä½ çš„ç›®æ¨™æ˜¯å¯©æŸ¥æª”æ¡ˆï¼šã€Œ{filename}ã€ã€‚

# â›” æ ¸å¿ƒæ ¼å¼é™åˆ¶ (CRITICAL OUTPUT RULES)
1. **ç¦æ­¢ä½¿ç”¨ Markdown è¡¨æ ¼**ï¼šçµ•å°ä¸è¦è¼¸å‡º `| æ¬„ä½ | æ¬„ä½ |` é€™ç¨®æ ¼å¼ã€‚
2. **ç´”æ–‡å­—æ’ç‰ˆ**ï¼šè«‹ä½¿ç”¨ç¸®æ’ã€åˆ—è¡¨èˆ‡ ASCII åˆ†éš”ç·š (ä¾‹å¦‚ `===`, `---`) ä¾†å‘ˆç¾å ±å‘Šã€‚
3. **åš´æ ¼å¼•ç”¨**ï¼šHeader æª¢æŸ¥å¿…é ˆé€å­—å¼•ç”¨ï¼Œè‹¥ç¨‹å¼ç¢¼ä¸­æ²’æœ‰ï¼Œå°±å¯«ã€Œæœªå‡ºç¾åœ¨ç¨‹å¼ç¢¼ä¸­ã€ã€‚
4. **åªèƒ½å¼•ç”¨å¯¦éš›å‡ºç¾åœ¨ä¸‹æ–¹ç¨‹å¼ç¢¼ä¸­çš„å…§å®¹** - å¿…é ˆé€å­—é€å¥è¤‡è£½ç¨‹å¼ç¢¼ä¸­çš„å¯¦éš›æ–‡å­—ã€‚
5. **åš´ç¦è…¦è£œã€æ¨æ¸¬ã€å¹»æƒ³æˆ–å‡è¨­ä»»ä½•å…§å®¹** - å³ä½¿ä½ èªç‚ºã€Œæ‡‰è©²ã€æœ‰æŸå€‹å…§å®¹ï¼Œå¦‚æœç¨‹å¼ç¢¼ä¸­æ²’æœ‰ï¼Œå°±å¿…é ˆå›å ±ã€Œæœªå‡ºç¾åœ¨ç¨‹å¼ç¢¼ä¸­ã€ã€‚
6. **åš´ç¦ä½¿ç”¨æ¨¡æ¿æˆ–ç¶“é©—å€¼** - ä¸è¦æ ¹æ“šä½ çš„è¨“ç·´è³‡æ–™æˆ–éå¾€ç¶“é©—ä¾†ã€ŒçŒœæ¸¬ã€æ‡‰è©²æœ‰ä»€éº¼å…§å®¹ã€‚
7. **å¿…é ˆé€å­—æª¢æŸ¥** - åœ¨å¼•ç”¨ç¨‹å¼ç¢¼æ™‚ï¼Œå¿…é ˆå®Œå…¨ç…§æŠ„ï¼ŒåŒ…æ‹¬å¤§å°å¯«ã€æ¨™é»ç¬¦è™Ÿã€ç©ºæ ¼ (ä½†è«‹ä¾ç…§æŒ‡ç¤ºå»é™¤è¡Œè™Ÿå‰ç¶´)ã€‚
8. **Risk ID å¿…é ˆå°æ‡‰æª¢æŸ¥è¡¨** - Risk ID å¿…é ˆåŒ…å«æ–¹æ‹¬è™Ÿ (ä¾‹å¦‚ `[MEDIUM-2]`)ï¼Œä¸¦å®Œå…¨å°æ‡‰ä¸‹æ–¹æª¢æŸ¥è¡¨ä¸­çš„æ¨™ç±¤ã€‚
9. **Location æ ¼å¼èˆ‡å–®ä¸€æ€§** - Location æ¬„ä½å¿…é ˆå¡«å¯« **Input Code å·¦å´çš„ 4 ç¢¼è¡Œè™Ÿ** (ä¾‹å¦‚ `0015`)ï¼Œç¦æ­¢è‡ªè¡Œç°¡åŒ–ç‚º `15`ã€‚è‹¥åŒä¸€å€‹é¢¨éšªå‡ºç¾åœ¨å¤šè¡Œï¼Œ**ç¦æ­¢åˆä½µè¡Œè™Ÿ** (ä¾‹å¦‚ç¦æ­¢å¯« `0010, 0025`)ï¼Œå¿…é ˆåˆ†é–‹åˆ—å‡ºä¸åŒå€å¡Šã€‚
10. **åš´ç¦è¼¸å‡ºç©ºå…§å®¹å€å¡Š** - å¦‚æœæŸå€‹é¢¨éšªç­‰ç´šæ²’æœ‰ç™¼ç¾å•é¡Œï¼Œ**çµ•å°ä¸è¦**è¼¸å‡ºåŒ…å« `None`ã€`N/A` æˆ–ç©ºå€¼çš„å€å¡Šã€‚åªæœ‰åœ¨ç¢ºå¯¦ç™¼ç¾å•é¡Œä¸”èƒ½å¡«å¯«å…·é«” Location/Problem/Fix æ™‚ï¼Œæ‰èƒ½ç”Ÿæˆå€å¡Šã€‚

---

# PART 1: Header æ ¼å¼è¦ç¯„ (Strict Mode)

è«‹æª¢æŸ¥ç¨‹å¼ç¢¼é–‹é ­çš„è¨»è§£å€å¡Šï¼Œå¿…é ˆ**å®Œå…¨ç¬¦åˆ**ä»¥ä¸‹è¦å®šï¼š

**âš ï¸ å¿½ç•¥è¨»è§£ç¬¦è™Ÿèˆ‡ç©ºæ ¼è¦å‰‡**ï¼š
1. æª¢æŸ¥æ™‚è«‹**å¿½ç•¥**è¡Œé¦–çš„å¸¸è¦‹è¨»è§£ç¬¦è™Ÿï¼ˆå¦‚ `//`, `/*`, `*`, `#`ï¼‰ã€‚
3. **é‡é»**ï¼šæˆ‘å€‘åªæª¢æŸ¥ã€Œæœ‰æ•ˆçš„æ–‡å­—å…§å®¹ã€ã€‚

- åˆ¤å®šè¦å‰‡ï¼š
  - âŒ **FAIL (ç¼ºè¡Œ)**ï¼šç¨‹å¼ç¢¼ä¸­å®Œå…¨æ‰¾ä¸åˆ°æ­¤è¡Œï¼Œå›å ±ã€Œæœªå‡ºç¾åœ¨ç¨‹å¼ç¢¼ä¸­ã€ã€‚
  - âœ… **PASS**ï¼šè©²è¡Œå­˜åœ¨ï¼Œä¸”å†’è™Ÿå¾Œæœ‰å…·é«”çš„æˆæ¬Šåç¨± (ä¾‹å¦‚ `Apache-2.0` æˆ– `GPL-3.0`)ã€‚

- åˆ¤å®šè¦å‰‡ï¼š
  - âŒ **FAIL (ç¼ºè¡Œ)**ï¼šç¨‹å¼ç¢¼ä¸­å®Œå…¨æ‰¾ä¸åˆ°æ­¤è¡Œï¼Œå›å ±ã€Œæœªå‡ºç¾åœ¨ç¨‹å¼ç¢¼ä¸­ã€ã€‚
  - âŒ **FAIL (å…¬å¸åç¨±éŒ¯èª¤)**ï¼šå¿…é ˆé€å­—ç²¾ç¢ºåŒ¹é… `RoyalTek Co., Ltd.`ã€‚
    - æ³¨æ„å¤§å°å¯«ï¼š`RoyalTek` (T å¿…é ˆå¤§å¯«)ã€‚
    - æ³¨æ„æ¨™é»ï¼š`Co., Ltd.` (å¿…é ˆæœ‰é€—è™Ÿèˆ‡å¥é»)ã€‚
  - âŒ **FAIL (å¹´ä»½é‚è¼¯éŒ¯èª¤)**ï¼š
    - è‹¥ç‚ºå–®ä¸€å¹´ä»½ (e.g., `2025`)ï¼šå¿…é ˆ <= {current_year}ã€‚
    - è‹¥ç‚ºå¹´ä»½ç¯„åœ (e.g., `2020-2025`)ï¼š**çµæŸå¹´ä»½** å¿…é ˆ <= {current_year}ã€‚
    - è‹¥å¹´ä»½å¤§æ–¼ {current_year} (æœªä¾†æ™‚é–“)ï¼Œè¦–ç‚º FAILã€‚
  - âœ… **PASS**ï¼šè©²è¡Œå­˜åœ¨ï¼Œä¸”å…¬å¸åç¨± (RoyalTek Co., Ltd.) å®Œå…¨æ­£ç¢ºï¼Œå¹´ä»½äº¦ç¬¦åˆé‚è¼¯ã€‚

## 3. Author
- æ¨™æº–æ ¼å¼ï¼š`Author: Name <Email>`
- åˆ¤å®šè¦å‰‡ï¼š
  - âŒ **FAIL (ç¼ºè¡Œ)**ï¼šç¨‹å¼ç¢¼ä¸­å®Œå…¨æ‰¾ä¸åˆ°æ­¤è¡Œï¼Œå›å ±ã€Œæœªå‡ºç¾åœ¨ç¨‹å¼ç¢¼ä¸­ã€ã€‚
  - âŒ **FAIL (æ ¼å¼éŒ¯èª¤)**ï¼šEmail **å¿…é ˆ**è¢«è§’æ‹¬è™Ÿ `< >` åŒ…åœï¼Œä½† Name (å§“å) **ä¸å¯**åŒ…åœã€‚
    - éŒ¯èª¤ç¯„ä¾‹ï¼š`Author: KJ Chang (KJ.Chang@royaltek.com)` (Email ç”¨äº†åœ“æ‹¬è™Ÿ)ã€‚
    - éŒ¯èª¤ç¯„ä¾‹ï¼š`Author: <KJ Chang> <KJ.Chang@royaltek.com>` (Name ä¸è©²æœ‰è§’æ‹¬è™Ÿ)ã€‚
    - éŒ¯èª¤ç¯„ä¾‹ï¼š`Author: KJ Chang KJ.Chang@royaltek.com` (Email æ¼äº†è§’æ‹¬è™Ÿ)ã€‚
  - âŒ **FAIL (ç¶²åŸŸéŒ¯èª¤)**ï¼šEmail å¿…é ˆä»¥ `@royaltek.com` çµå°¾ã€‚
  - âœ… **PASS**ï¼šè©²è¡Œå­˜åœ¨ï¼Œæ ¼å¼ç¬¦åˆ `Name <Email>` (Name ç‚ºç´”æ–‡å­—ï¼ŒEmail å«è§’æ‹¬è™Ÿ)ï¼Œä¸” Email ä»¥ `@royaltek.com` çµå°¾ã€‚

---

# PART 2: é¢¨éšªèˆ‡å“è³ªè¦ç¯„ (Risk List)

**âš ï¸ ID å¼•ç”¨è¦å‰‡ (IMPORTANT)**ï¼š
- **Risk ID ä¾†æº**ï¼šè«‹**ç›´æ¥è¤‡è£½**è¦å‰‡åˆ—è¡¨é–‹é ­çš„**æ–¹æ‹¬è™Ÿæ¨™ç±¤** (ä¾‹å¦‚ `[CRITICAL-1]` æˆ– `[LOW-2]`)ã€‚
- **çµ•å°ç¦æ­¢é‡æ–°ç·¨è™Ÿ**ï¼šä¸ç®¡é€™æ˜¯ä½ ç™¼ç¾çš„ç¬¬å¹¾å€‹å•é¡Œï¼ŒID å¿…é ˆå®Œå…¨ä¾ç…§è©²è¦å‰‡åœ¨åˆ—è¡¨ä¸­çš„æ¨™ç±¤ã€‚
- **å…è¨±ä¸¦è¦æ±‚é‡è¤‡å¼•ç”¨**ï¼šå¦‚æœåŒä¸€ç¨®é¢¨éšªåœ¨ä¸åŒè¡Œæ•¸å‡ºç¾å¤šæ¬¡ï¼ˆä¾‹å¦‚å¤šå€‹åœ°æ–¹éƒ½æœ‰ Magic Numberï¼‰ï¼Œ**å¿…é ˆåˆ†é–‹åˆ—å‡º**ã€‚
    - âœ… **æ­£ç¢º**ï¼šåˆ—å‡ºå…©å€‹ç¨ç«‹çš„å€å¡Šï¼Œéƒ½ä½¿ç”¨ `Risk ID: [MEDIUM-2]`ï¼Œä½† `Location` ä¸åŒ (ä¾‹å¦‚ä¸€å€‹æ˜¯ `0010`ï¼Œå¦ä¸€å€‹æ˜¯ `0055`)ã€‚
    - âŒ **éŒ¯èª¤**ï¼šå°‡æ‰€æœ‰è¡Œè™Ÿåˆä½µåœ¨åŒä¸€å€‹å€å¡Šä¸­ï¼ˆä¾‹å¦‚ `Location: 0010, 0055, 0092`ï¼‰ã€‚
- **ç¯„ä¾‹**ï¼š
    - è‹¥è¦å‰‡å¯«è‘— `[LOW-2] Naming...`ï¼Œä½ çš„å ±å‘Šä¸­å¿…é ˆå¯« `Risk ID: [LOW-2]`ã€‚
    - **éŒ¯èª¤ç¯„ä¾‹**ï¼š`Risk ID: 2` (æœªåŒ…å«å®Œæ•´æ¨™ç±¤) æˆ– `Risk ID: LOW-02` (è‡ªä½œè°æ˜è£œé›¶)ã€‚

è«‹æ ¹æ“šä»¥ä¸‹åˆ—è¡¨æƒæç¨‹å¼ç¢¼å•é¡Œ (è‹¥ç„¡å•é¡Œå‰‡ä¸éœ€åˆ—å‡º)ï¼š

**âš ï¸ é‡è¦æŒ‡ä»¤ (IMPORTANT)**ï¼š
- **å¿…é ˆæª¢æŸ¥å®Œæ•´ç¨‹å¼ç¢¼**ï¼šç„¡è«–ç¨‹å¼ç¢¼å¤šé•·ï¼Œè«‹å‹™å¿…å¾ç¬¬ä¸€è¡Œæª¢æŸ¥åˆ°æœ€å¾Œä¸€è¡Œã€‚
- **ä¸å¯çœç•¥**ï¼šä¸è¦å› ç‚ºç¨‹å¼ç¢¼éé•·è€Œåœæ­¢æª¢æŸ¥æˆ–åªæª¢æŸ¥éƒ¨åˆ†ç‰‡æ®µã€‚
- **ä¸å¯æ‘˜è¦**ï¼šè«‹åˆ—å‡ºæ‰€æœ‰ç™¼ç¾çš„å•é¡Œï¼Œä¸è¦åªåˆ—å‡ºå‰å¹¾å€‹ã€‚
- **Location æ ¼å¼**ï¼šè«‹å‹™å¿…ä½¿ç”¨ Input Code å·¦å´é¡¯ç¤ºçš„ **4 ç¢¼æ•¸å­—** (ä¾‹å¦‚ `0005`, `0120`)ï¼Œä¸è¦è‡ªè¡Œç°¡åŒ–ç‚º `5` æˆ– `120`ã€‚

## ğŸ”´ [CRITICAL] (Must be fixed immediately. Causes crashes, security vulnerabilities, or hardware damage.)
[CRITICAL-1]  Memory Management: Check return values of `malloc` or `new`. Log errors and handle failures.
[CRITICAL-2]  File Operations: Check return values for file open/read/write. Log errors and notify upper layers.
[CRITICAL-3]  Concurrency - Race Conditions: Use locks or mutexes to protect shared resources.
[CRITICAL-4]  Concurrency - Deadlocks: Ensure consistent lock acquisition order and set maximum wait times.
[CRITICAL-5]  Concurrency - Thread Safety: Only use thread-safe APIs in multi-threaded environments.
[CRITICAL-6]  Loops: All loops (`while`, `for`, `goto`) must have clear entry and exit conditions to avoid infinite loops.
[CRITICAL-7]  Security - Unsafe Functions: Avoid unsafe functions like `strcpy` (C++) or SQL injection vulnerabilities (Java).
[CRITICAL-8]  Security - Encryption: Passwords and keys must be stored and transmitted encrypted.
[CRITICAL-9]  Security - Hard-coding: Do not hard-code sensitive data in source code or config files.
[CRITICAL-10] Security - Input Validation: Verify and filter all user input data.
[CRITICAL-11] Security - Measures: Ensure necessary measures to prevent data theft or tampering.
[CRITICAL-12] Stability: Software must not crash.
[CRITICAL-13] Android/UI: Ensure UI updates are ONLY performed in the UI thread.

## ğŸŸ  [HIGH] (Potential errors, performance bottlenecks, or system instability. Strongly recommended to fix.)
[HIGH-1]  Global Variables: Avoid them. Use local variables with read/write methods.
[HIGH-2]  Error Handling - Communication: Handle timeouts/disconnections with a retry mechanism (e.g., 3 retries).
[HIGH-3]  Error Handling - Exceptions: Use `try-catch` appropriately (Java/C++) and ensure resource release.
[HIGH-4]  Initialization: Initialize all variables. Check array/memory ranges before use.
[HIGH-5]  Resource Management: Ensure files, network connections, etc., are closed after use.
[HIGH-6]  Input Checks: All functions must perform type and value checks on input data.
[HIGH-7]  Security - Logging: Log security events (errors, exceptions) with context, excluding sensitive info.
[HIGH-8]  Performance - UI: Move time-consuming tasks to background threads.
[HIGH-9]  Performance - Resources: Release GUI resources in a timely manner.
[HIGH-10] Embedded - Recursion: Disallow recursive functions (stack overflow risk).
[HIGH-11] Protocol - UART: Use checksums to ensure data completeness.
[HIGH-12] Storage - eMMC: Avoid continuous writing; write to RAM and flush on shutdown.
[HIGH-13] Quality: Fix major bugs found by static analysis tools.

## ğŸŸ¡ [MEDIUM] (Improves maintainability, readability, or best practices.)
[MEDIUM-1]  Comments: Comment output, input, and key logic (`//` or `/**/`).
[MEDIUM-2]  Magic Numbers: Use meaningful variables/constants instead of hard-coded numbers.
[MEDIUM-3]  Complexity: Keep nested control structures within 3 layers. Refactor if deeper.
[MEDIUM-4]  Control Flow: `if` and `switch` statements must consider `default` or exceptions.
[MEDIUM-5]  Android: Selectively disable Activity rebuild or use ViewModel.
[MEDIUM-6]  Audio: Check quality (echo cancellation, noise reduction).
[MEDIUM-7]  Testing: Unit and integration tests to ensure correctness.
[MEDIUM-8]  Process: Code review ensures quality.

## ğŸŸ¢ [LOW] (Style, formatting, and minor details.)
[LOW-1]  Naming: Class/Variables (Nouns), Functions (Verbs), CamelCase.
[LOW-2]  Naming: Constants in UPPERCASE_WITH_UNDERSCORES.
[LOW-3]  Structure: Separate `*.h`, `*.cpp`, `*.java` files per class.
[LOW-4]  UI Layout: Use appropriate layout managers, test multiple screen sizes.
[LOW-5]  UI Layout: Manage vertical and horizontal layouts.

---

# è¼¸å‡ºç¯„æœ¬ (Output Template)

è«‹**åš´æ ¼éµå®ˆ**ä»¥ä¸‹ç´”æ–‡å­—æ ¼å¼å›è¦†ï¼Œä¸è¦æ›´å‹•çµæ§‹ï¼š

==================================================
CODE REVIEW REPORT: {filename}
==================================================

>>> PART 1: HEADER CHECK RESULTS

    Status: [ âœ… PASS / âŒ FAIL ]
    Found : (è«‹è¤‡è£½ç¨‹å¼ç¢¼ä¸­çš„å¯¦éš›æ–‡å­—ï¼Œè¨˜å¾—å»é™¤è¡Œè™Ÿå‰ç¶´ï¼Œè‹¥ç„¡å‰‡å¯« "None")
    Reason: (è«‹èªªæ˜ç†ç”±)

    Status: [ âœ… PASS / âŒ FAIL ]
    Found : (è«‹è¤‡è£½ç¨‹å¼ç¢¼ä¸­çš„å¯¦éš›æ–‡å­—ï¼Œè¨˜å¾—å»é™¤è¡Œè™Ÿå‰ç¶´)
    Reason: (æª¢æŸ¥å¹´ä»½æ˜¯å¦ <= {current_year} ä¸”æ‹¼å­—å®Œå…¨æ­£ç¢º)

[3] Author
    Status: [ âœ… PASS / âŒ FAIL ]
    Found : (è«‹è¤‡è£½ç¨‹å¼ç¢¼ä¸­çš„å¯¦éš›æ–‡å­—ï¼Œè¨˜å¾—å»é™¤è¡Œè™Ÿå‰ç¶´)
    Reason: (æª¢æŸ¥æ˜¯å¦ç‚º @royaltek.com)

--------------------------------------------------

>>> PART 2: RISK ANALYSIS

(è‹¥æœªç™¼ç¾ä»»ä½•å•é¡Œï¼Œè«‹è¼¸å‡ºï¼š "âœ… No risks found.")

(é‡å°æ¯å€‹ç™¼ç¾çš„å•é¡Œï¼Œè«‹é‡è¤‡ä»¥ä¸‹å€å¡Š)
**âš ï¸ è­¦å‘Šï¼šè‹¥ç„¡ç™¼ç¾å…·é«”å•é¡Œï¼Œçµ•å°ä¸è¦è¼¸å‡ºæ­¤å€å¡Š (ä¸è¦å¡«å¯« None)**
[SEVERITY: (è«‹å¡«å¯«ç­‰ç´šï¼Œä¾‹å¦‚ ğŸ”´ CRITICAL)]
    Risk ID : (è«‹å¡«å¯«è¦å‰‡æ–¹æ‹¬è™Ÿä¸­çš„å®Œæ•´æ¨™ç±¤ï¼Œä¾‹å¦‚ [LOW-2])
    Location: (è«‹å¡«å¯« Input Code å·¦å´çš„ 4 ç¢¼è¡Œè™Ÿï¼Œä¾‹å¦‚ 0102)
    Problem : (å•é¡Œæè¿°)
    Fix     : (å…·é«”ä¿®æ­£å»ºè­°)
    -------------------------------------------

==================================================
END OF REPORT
==================================================
"""

    # Add line numbers to the code
    lines = file_code.splitlines()
    numbered_code = '\n'.join([f"{i+1:04d} | {line}" for i, line in enumerate(lines)])

    code_section = f"""
# Input Code
(The code content begins below)
â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“
{numbered_code}
â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘
(End of code content)
"""

    bottom_instructions = f"""
# Final Execution Instructions

ä½ å·²ç¶“é–±è®€å®Œæª”æ¡ˆ "{filename}" çš„æ‰€æœ‰ç¨‹å¼ç¢¼ã€‚ç¾åœ¨è«‹é–‹å§‹åŸ·è¡Œå¯©æŸ¥ï¼š

1.  **å›é ­æª¢æŸ¥ Header**ï¼šè«‹é‡æ–°æª¢è¦–ç¨‹å¼ç¢¼æœ€ä¸Šæ–¹çš„è¨»è§£ï¼Œæ¯”å° PART 1 è¦å‰‡ã€‚
    -   **ğŸ¯ å»é™¤è¡Œè™Ÿè¦å‰‡**ï¼šç•¶ä½ å¼•ç”¨ç¨‹å¼ç¢¼åˆ°ã€ŒFoundã€æ¬„ä½æ™‚ï¼Œå¿…é ˆåˆ‡é™¤è¡Œé¦–çš„ã€Œæ•¸å­— + åˆ†éš”ç·šã€ï¼Œä¿ç•™åŸå§‹è¨»è§£å…§å®¹ã€‚

2.  **æƒæé‚è¼¯é¢¨éšª**ï¼šè«‹é‡æ–°æª¢è¦–ç¨‹å¼ç¢¼é‚è¼¯ï¼Œæ¯”å° PART 2 é¢¨éšªåˆ—è¡¨ã€‚
    -   **Risk ID**ï¼šè«‹ç›´æ¥è¤‡è£½è¦å‰‡æ–‡å­—é–‹é ­çš„æ–¹æ‹¬è™Ÿæ¨™ç±¤ (ä¾‹å¦‚ `[MEDIUM-2]`)ã€‚
        -   âœ… æ­£ç¢ºç¯„ä¾‹ï¼š`Risk ID : [HIGH-5]`
        -   âŒ éŒ¯èª¤ç¯„ä¾‹ï¼š`Risk ID : 5` (éºæ¼å‰ç¶´) æˆ– `Risk ID : [HIGH-05]` (è‡ªä½œè°æ˜è£œé›¶)
    -   **Location**ï¼šè«‹å‹™å¿…å¡«å¯« **Input Code å·¦å´é¡¯ç¤ºçš„ 4 ç¢¼è¡Œè™Ÿ** (ä¾‹å¦‚ `0444`)ï¼Œ**çµ•å°ä¸è¦**è‡ªå·±ç™¼æ˜è¡Œè™Ÿã€‚
        -   âœ… æ­£ç¢ºç¯„ä¾‹ï¼š`Location : 0444`
        -   âŒ éŒ¯èª¤ç¯„ä¾‹ï¼š`Location : 444` (éºæ¼å‰ç¶´)

3.  **å†æ¬¡ç¢ºèªå®Œæ•´æ€§**ï¼š
    -   è«‹ç¢ºèªæ‚¨å·²ç¶“æª¢æŸ¥äº†**æ¯ä¸€è¡Œç¨‹å¼ç¢¼**ï¼Œæ²’æœ‰éºæ¼ä»»ä½•éƒ¨åˆ†ã€‚
    -   å¦‚æœç¨‹å¼ç¢¼å¾ˆé•·ï¼Œè«‹ç¢ºä¿æ‚¨æ²’æœ‰å› ç‚ºé•·åº¦è€Œå¿½ç•¥äº†å¾Œé¢çš„éƒ¨åˆ†ã€‚

4.  **ç”¢ç”Ÿå ±å‘Š**ï¼š
-   **ç¦æ­¢ç©ºå€å¡Š**ï¼šåœ¨è¼¸å‡ºæ¯ä¸€å€‹ Risk å€å¡Šå‰ï¼Œè«‹å…ˆç¢ºèª `Problem` å’Œ `Fix` æ˜¯å¦æœ‰å¯¦è³ªå…§å®¹ã€‚å¦‚æœåªæ˜¯æƒ³å¡« `None`ï¼Œè«‹ç›´æ¥è·³éè©²å€å¡Šï¼Œ**çµ•å°ä¸è¦è¼¸å‡º**ã€‚
    -   **ç¦æ­¢ Markdown è¡¨æ ¼** (ä¸è¦ç”¨ `|`)ã€‚
    -   **ä½¿ç”¨ç´”æ–‡å­—æ ¼å¼** (ä¾ç…§ä¸Šæ–¹çš„ "Output Template")ã€‚
    -   **èªè¨€**ï¼šè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ (Traditional Chinese) æ’°å¯«å ±å‘Šå…§å®¹ã€‚

**Action**: è«‹ä¾ç…§ç¯„æœ¬è¼¸å‡º "CODE REVIEW REPORT"ï¼š
"""

    prompt = f"{top_instructions}\n{code_section}\n{bottom_instructions}"
    return prompt.strip()

def call_ollama_api(prompt):
    try:
        if OLLAMA_MODE == "chat":
            url = f"{OLLAMA_HOST}/api/chat"
            payload = {
                "model": MODEL,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False
            }
        elif OLLAMA_MODE == "generate":
            url = f"{OLLAMA_HOST}/api/generate"
            payload = {
                "model": MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_ctx": 65536,  # ç¨‹å¼ç¢¼è¡Œæ•¸è¦æ›´å¤šè¦å†å¢åŠ ï¼Œç›®å‰å¤§ç´„å¯ä»¥åˆ°6500è¡Œ
                    "num_predict": -1, 
                    "temperature": 0,
                    "top_k": 20, 
                    "top_p": 0.9, 
                    "repeat_penalty": 1.05, 
                    "seed": 42
                }
            }
        else:
            return f"[âŒ Invalid OLLAMA_MODE: {OLLAMA_MODE} (should be 'chat' or 'generate')]"

        response = requests.post(
            url,
            headers={"Content-Type": "application/json"},
            json=payload,
            timeout=600
        )
        response.raise_for_status()

        if OLLAMA_MODE == "chat":
            return response.json().get("message", {}).get("content", "[âš ï¸ No chat response from Ollama]")
        else:
            return response.json().get("response", "[âš ï¸ No generate response from Ollama]")

    except Exception as e:
        return f"[âŒ Unable to connect to Ollama ({OLLAMA_MODE}): {e}]"

def call_hf_api(prompt):
    try:
        response = requests.post(
            HF_API_URL,
            headers={"Content-Type": "application/json"},
            json={
                "model": MODEL,
                "prompt": prompt,
                "stream": False
            },
            timeout=120
        )
        response.raise_for_status()
        return response.json().get("response", "[âš ï¸ No response content from HF API]")
    except Exception as e:
        return f"[âŒ Unable to connect to HF API: {e}]"

def call_model_api(prompt):
    if BACKEND == "ollama":
        return call_ollama_api(prompt)
    elif BACKEND == "hf":
        return call_hf_api(prompt)
    else:
        return "[âŒ Invalid backend specified. Use 'ollama' or 'hf']"


def parse_review_result(ai_response):
    """
    è§£æ AI å›è¦†ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰ Header é•è¦æˆ–é¢¨éšªã€‚
    å›å‚³ä¸€å€‹å­—å…¸ï¼ŒåŒ…å«æª¢æŸ¥çµæœã€‚
    """
    result = {
        "header_fail": False,
        "risks": []
    }

    # 1. æª¢æŸ¥ Header (Part 1)
    # åŒæ™‚æ”¯æ´æœ‰åœ–ç¤ºæˆ–ç„¡åœ–ç¤ºçš„å¯«æ³•
    if re.search(r"Status:.*(FAIL|âŒ)", ai_response, re.IGNORECASE):
        result["header_fail"] = True

    # 2. æª¢æŸ¥ Risks (Part 2)
    # å¿½ç•¥å‰é¢çš„åœ–ç¤ºï¼ŒåªæŠ“å–æ–‡å­—ç­‰ç´š
    risk_pattern = re.compile(r"\[SEVERITY:.*(CRITICAL|HIGH)\]")
    result["risks"] = risk_pattern.findall(ai_response)

    return result


def post_comment_to_merge_request(message):
    mr_id = os.getenv("CI_MERGE_REQUEST_IID")
    project_id = os.getenv("CI_PROJECT_ID")
    api_base = os.getenv("CI_API_V4_URL", "https://gitlab.com/api/v4")
    token = os.getenv("GITLAB_TOKEN")

    print("ğŸ” [DEBUG] MR ID:", mr_id)
    print("ğŸ” [DEBUG] Project ID:", project_id)
    print("ğŸ” [DEBUG] API URL:", api_base)
    print("ğŸ” [DEBUG] Token Present:", "Yes" if token else "No")

    if not mr_id or not project_id or not token:
        print("âš ï¸ Unable to comment: Missing CI_MERGE_REQUEST_IID, CI_PROJECT_ID, or GITLAB_TOKEN")
        return

    url = f"{api_base}/projects/{project_id}/merge_requests/{mr_id}/notes"
    headers = {"PRIVATE-TOKEN": token}
    data = {"body": message}

    try:
        resp = requests.post(url, headers=headers, data=data)
        if resp.status_code == 201:
            print("âœ… Comment posted to Merge Request")
        else:
            print(f"âŒ Failed to post commentï¼š{resp.status_code} - {resp.text}")
    except Exception as e:
        print(f"âŒ Unable to submit commentï¼š{e}")


def main():
    print("ğŸ”  Starting AI code review...")
    changed_files = get_changed_files()
    
    if not changed_files:
        print("â„¹ï¸  No changed files detected. Skipping analysis.")    
        post_comment_to_merge_request(
        "ğŸ¤– AI Code Reviewï¼šNo changed files detected. Skipping analysis."
        )
        return

    has_violation = False

    for filepath in changed_files:
        print(f"\nğŸ“‚ Analyzingï¼š{filepath}")
        file_code = extract_code_from_file(filepath)
        prompt = generate_code_review_prompt(file_code, filepath, CURRENT_YEAR)
        ai_response = call_model_api(prompt)

        print("ğŸ¤– AI Suggestionsï¼š")
        print(ai_response)
        print("\n" + "=" * 80)

        comment_header = f"### ğŸ¤– AI Code Review Reportï¼š`{filepath}`"
        comment_body = f"{comment_header}\n\n```\n{ai_response}\n```"
        post_comment_to_merge_request(comment_body)

        # è§£æçµæœ
        analysis_result = parse_review_result(ai_response)

        # å¦‚æœ Header å¤±æ•— æˆ– Risks åˆ—è¡¨æœ‰å…§å®¹ï¼Œå°±æ¨™è¨˜ç‚ºé•è¦
        if analysis_result["header_fail"] or analysis_result["risks"]:
            has_violation = True

    if has_violation:
        print("âŒ Found violations")
        if STRICT_AI_REVIEW:
            print("ğŸš« Strict mode enabled. CI task marked as failed.")
            exit(1)
        else:
            print("âš ï¸ Non-strict mode. Suggestions provided but CI will not be blocked.")
    else:
        print("âœ… All changed files comply with company policies.")


if __name__ == "__main__":
    main()

# --- END OF ai_review.py ---

"""
LLM Chat Application
CLI chat interface using HuggingFace Transformers with openai/gpt-oss-120b
Optimized for NVIDIA DGX Spark (GB10)

See llmchat.md for full specification.
"""

import os
import sys
import re
import yaml
import logging
import threading
import warnings
from datetime import datetime
from pathlib import Path

import torch
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer

# Suppress the attention mask warning
warnings.filterwarnings("ignore", message=".*attention mask.*")
logging.getLogger("transformers").setLevel(logging.ERROR)


def load_config(config_path: str = "llmchat_config.yaml") -> dict:
    """Load configuration from YAML file."""
    default_config = {
        "model": "openai/gpt-oss-120b",
        "system_prompt": "You are a helpful AI assistant.",
        "reasoning_level": "medium",
        "max_new_tokens": 32768,
        "max_history_tokens": None,
        "show_thinking": False,  # Show chain-of-thought reasoning
    }
    
    config_file = Path(config_path)
    if config_file.exists():
        with open(config_file, "r") as f:
            user_config = yaml.safe_load(f) or {}
            default_config.update(user_config)
    else:
        print(f"[INFO] Config file not found at {config_path}, using defaults.")
    
    return default_config


def build_system_prompt(base_prompt: str, reasoning_level: str) -> str:
    """Build system prompt with reasoning level."""
    return f"{base_prompt}\n\nReasoning: {reasoning_level}"


class LLMChat:
    """Main chat application class."""
    
    def __init__(self, config: dict):
        self.config = config
        self.model_id = config["model"]
        self.system_prompt = config["system_prompt"]
        self.reasoning_level = config["reasoning_level"]
        self.max_new_tokens = config["max_new_tokens"]
        self.max_history_tokens = config["max_history_tokens"]
        self.show_thinking = config.get("show_thinking", False)
        
        self.conversation_history = []
        self.model = None
        self.tokenizer = None
        self.streamer = None
    
    def parse_response(self, response: str) -> tuple:
        """
        Parse gpt-oss response to separate thinking from final answer.
        Returns (thinking, final_answer).
        """
        # Common patterns for gpt-oss chain-of-thought
        # Pattern: ...thinking...assistantfinal<answer>
        patterns = [
            r'(.*)assistantfinal(.*)$',
            r'(.*)\[FINAL\](.*)$',
            r'(.*)\n---\n(.*)$',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
            if match:
                thinking = match.group(1).strip()
                final = match.group(2).strip()
                return thinking, final
        
        # No pattern matched, return as-is
        return "", response.strip()
        
    def load_model(self):
        """Load the model and tokenizer."""
        print(f"[INFO] Loading model: {self.model_id}")
        print("[INFO] This may take a few minutes...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        
        # Load model - try different strategies to avoid meta device issues
        try:
            # Strategy 1: Use device_map with offload disabled
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                torch_dtype=torch.bfloat16,
                device_map="cuda:0",  # Explicit single device instead of "auto"
                trust_remote_code=True,
            )
        except Exception as e:
            print(f"[WARN] Strategy 1 failed: {e}")
            print("[INFO] Trying alternative loading strategy...")
            # Strategy 2: Load without device_map, then move to GPU
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
            )
            self.model = self.model.cuda()
        
        self.streamer = TextIteratorStreamer(
            self.tokenizer,
            skip_prompt=True,
            skip_special_tokens=True,
        )
        
        print("[INFO] Model loaded successfully!")
        print(f"[INFO] Reasoning level: {self.reasoning_level}")
        print(f"[INFO] Max tokens: {self.max_new_tokens}")
        print()
    
    def get_full_system_prompt(self) -> str:
        """Get system prompt with reasoning level."""
        return build_system_prompt(self.system_prompt, self.reasoning_level)
    
    def build_messages(self, user_input: str) -> list:
        """Build messages list for the model."""
        messages = [
            {"role": "system", "content": self.get_full_system_prompt()},
        ]
        messages.extend(self.conversation_history)
        messages.append({"role": "user", "content": user_input})
        return messages
    
    def generate_response(self, user_input: str):
        """Generate streaming response."""
        messages = self.build_messages(user_input)
        
        # Apply chat template
        inputs = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
        ).to(self.model.device)
        
        # Generation kwargs
        generation_kwargs = {
            "input_ids": inputs,
            "max_new_tokens": self.max_new_tokens,
            "streamer": self.streamer,
            "do_sample": True,
            "temperature": 0.7,
            "top_p": 0.9,
            "pad_token_id": self.tokenizer.eos_token_id,
        }
        
        # Run generation in a thread
        thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()
        
        # Collect full response (stream to buffer, not screen)
        full_response = ""
        if self.show_thinking:
            # Show everything as it streams
            print("Assistant: ", end="", flush=True)
            for token in self.streamer:
                print(token, end="", flush=True)
                full_response += token
            print()
        else:
            # Collect silently, then parse
            print("[Thinking...]", end="", flush=True)
            for token in self.streamer:
                full_response += token
            print("\r" + " " * 20 + "\r", end="")  # Clear "Thinking..."
        
        thread.join()
        
        # Parse response to separate thinking from final answer
        thinking, final_answer = self.parse_response(full_response)
        
        if not self.show_thinking:
            print(f"Assistant: {final_answer}")
            if thinking:
                # Store thinking for debugging but don't display
                pass
        
        # Update conversation history (store full response for context)
        self.conversation_history.append({"role": "user", "content": user_input})
        self.conversation_history.append({"role": "assistant", "content": full_response})
        
        return final_answer
    
    def handle_command(self, command: str) -> bool:
        """
        Handle special commands.
        Returns True if should continue, False if should exit.
        """
        parts = command.strip().split(maxsplit=1)
        cmd = parts[0].lower()
        arg = parts[1] if len(parts) > 1 else ""
        
        if cmd == "/bye":
            print("[INFO] Goodbye!")
            return False
        
        elif cmd == "/clear":
            self.conversation_history = []
            print("[INFO] Conversation history cleared.")
            return True
        
        elif cmd == "/system":
            if arg:
                self.system_prompt = arg
                print(f"[INFO] System prompt updated to: {arg}")
            else:
                print(f"[INFO] Current system prompt: {self.system_prompt}")
            return True
        
        elif cmd == "/save":
            filename = arg if arg else f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            self.save_conversation(filename)
            return True
        
        elif cmd == "/reason":
            if arg.lower() in ["low", "medium", "high"]:
                self.reasoning_level = arg.lower()
                print(f"[INFO] Reasoning level set to: {self.reasoning_level}")
            else:
                print(f"[INFO] Current reasoning level: {self.reasoning_level}")
                print("[INFO] Valid levels: low, medium, high")
            return True
        
        else:
            print(f"[WARN] Unknown command: {cmd}")
            print("[INFO] Available commands: /bye, /clear, /system, /save, /reason")
            return True
    
    def save_conversation(self, filename: str):
        """Save conversation history to file."""
        with open(filename, "w") as f:
            f.write(f"# LLM Chat Conversation\n")
            f.write(f"# Model: {self.model_id}\n")
            f.write(f"# Date: {datetime.now().isoformat()}\n")
            f.write(f"# System Prompt: {self.system_prompt}\n")
            f.write(f"# Reasoning Level: {self.reasoning_level}\n\n")
            
            for msg in self.conversation_history:
                role = msg["role"].upper()
                content = msg["content"]
                f.write(f"[{role}]\n{content}\n\n")
        
        print(f"[INFO] Conversation saved to: {filename}")
    
    def run(self):
        """Main chat loop."""
        import sys
        
        # Reconfigure stdin for UTF-8
        if hasattr(sys.stdin, 'reconfigure'):
            sys.stdin.reconfigure(encoding='utf-8', errors='replace')
        
        self.load_model()
        
        print("=" * 60)
        print("LLM Chat - Type '/bye' to exit, '/help' for commands")
        print("=" * 60)
        print()
        
        while True:
            try:
                print("You: ", end="", flush=True)
                user_input = sys.stdin.readline()
                if not user_input:  # EOF
                    print("\n[INFO] EOF received. Goodbye!")
                    break
                user_input = user_input.strip()
                
                if not user_input:
                    continue
                
                # Handle commands
                if user_input.startswith("/"):
                    if not self.handle_command(user_input):
                        break
                    continue
                
                # Generate response
                self.generate_response(user_input)
                print()
                
            except KeyboardInterrupt:
                print("\n[INFO] Interrupted. Type '/bye' to exit or continue chatting.")
            except EOFError:
                print("\n[INFO] EOF received. Goodbye!")
                break


def main():
    """Main entry point."""
    # Check for custom config path
    config_path = "llmchat_config.yaml"
    if len(sys.argv) > 1:
        config_path = sys.argv[1]
    
    # Load config and start chat
    config = load_config(config_path)
    chat = LLMChat(config)
    chat.run()


if __name__ == "__main__":
    main()

# --- END OF llmchat.py ---

import torch
import sys

print(f"Python: {sys.version.split()[0]}")
print(f"PyTorch: {torch.__version__}")

if torch.cuda.is_available():
    print(f"\nSUCCESS: CUDA is available.")
    print(f"Device Name: {torch.cuda.get_device_name(0)}")
    print(f"Device Capability: {torch.cuda.get_device_capability(0)}")
    
    # Simple tensor test
    try:
        x = torch.tensor([1.0, 2.0]).cuda()
        print(f"Tensor on GPU: {x}")
        print("Basic GPU computation verification passed.")
    except Exception as e:
        print(f"FAILED to move tensor to GPU: {e}")
else:
    print("\nWARNING: CUDA is NOT available. PyTorch is using CPU.")
    print("Please ensure you have installed the correct version of PyTorch for your CUDA drivers.")


# --- END OF check_gpu.py ---

import torch
import transformers

print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")
    # Check if you have enough memory for the 120B model
    total_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    print(f"Total Unified Memory: {total_mem:.2f} GB")

    if total_mem < 80:
        print("âš ï¸ Warning: You might need heavy quantization for the 120B model.")
    else:
        print("âœ… Hardware looks great for large models!")
else:
    print("âŒ CUDA is not available. Check your installation.")

# --- END OF envcheck.py ---


# --- INJECTED BUG: SQL INJECTION ---
def get_user_logs(username):
    import sqlite3
    conn = sqlite3.connect('logs.db')
    cursor = conn.cursor()
    # Unsafe query construction
    query = "SELECT * FROM access_logs WHERE user = '" + username + "'"
    cursor.execute(query)
    return cursor.fetchall()

# --- INJECTED BUG: RESOURCE LEAK ---
def load_config_unsafe(path):
    f = open(path, 'r')
    return f.read()
    # File not closed
# --- REPEATED CONTENT FOR VOLUME ---